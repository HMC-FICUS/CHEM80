{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1udD5v1As1QrhRPmfJgzPRdTx992n7sgn","timestamp":1709724549125},{"file_id":"1CDW0YYejxxBwaBHZ3yvlCrF9jP_xOlFB","timestamp":1709552969718},{"file_id":"1KJVhs9Wmx7ff7NyPytj8p9v7u9Zmbhgx","timestamp":1708040382739},{"file_id":"17cUJAmt-Ze--6OfutYvzwEV3Go0C1b9e","timestamp":1706638038791},{"file_id":"1-osvtQCGVecL-32l3fHgJmn2ONyfjHqn","timestamp":1696907647526}],"toc_visible":true,"authorship_tag":"ABX9TyNYwlO9/Io0wniT4awYYwNo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CHEM 60 - March 4th, 2024 (Image Processing)\n","\n","Today, we get to play with a new kind of data! Images!\n","\n","To get started, click on '**File**' in the left menu, then '**Save a copy in Drive**' to ensure you are editing *your* version of this assignment (if you don't, your changes won't be saved!). After you click '**Save a copy in Drive**' a popup that says **Notebook copy complete** should appear, and it may ask you to <font color='blue'>**Open in a new tab**</font>. When open, your new file will be named `Copy of CHEM60_Class_14_....ipynb` (you may want to rename it before/after you move it to your chosen directory)."],"metadata":{"id":"J8F8-SnWTzNC"}},{"cell_type":"markdown","source":["#Imports\n","\n","Here are the Python imports that we will need today plus the usual formatting things.\n","\n","Run the below code block to get started."],"metadata":{"id":"Q_h-NNX5OpZ5"}},{"cell_type":"code","source":["# Standard library imports\n","import copy\n","import math as m\n","\n","# Third party imports\n","import cv2\n","import matplotlib.patches as patches\n","import matplotlib.gridspec as gridspec\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from PIL import ImageOps\n","from PIL.PngImagePlugin import PngImageFile, PngInfo\n","\n","\n","# This part of the code block is telling matplotlib to make certain font sizes exra, extra large by default\n","# Here is where I list what parametres I want to set new defaults for\n","params = {'legend.fontsize': 'xx-large',\n","         'axes.labelsize': 'xx-large',\n","         'axes.titlesize':'xx-large',\n","         'xtick.labelsize':'xx-large',\n","         'ytick.labelsize':'xx-large'}\n","# This line updates the default parameters of pyplot (to use our larger fonts)\n","plt.rcParams.update(params)"],"metadata":{"id":"_Jcu7W31OpkR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, mount the Drive. You've done this every week now! If the details of things like imports or data access need to be clarified, go back and check out the [class 0](https://colab.research.google.com/drive/1q96pdc5CBfjhqkALe-ohqPJwNMcXzwqS?usp=share_link) notebook on this."],"metadata":{"id":"7LHeYOwVlONb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"Wo56sTodlOVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assuming you ended up with `Mounted at /content/gdrive`, you're good to move on!"],"metadata":{"id":"bvGbLtPXDdWW"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"IYM6htFuyoML"}},{"cell_type":"markdown","source":["# The Motivation\n","\n","Today's class is another special one - a data story from our chemistry department. Prof. Haushalter shared a lovely result from his research group and details on their main experimental technique.\n","\n",">  [T]he majority of my research lab's raw data comes from gel electrophoresis, which separates proteins or nucleic acids into distinct bands which can be visualized by staining and then imaged with a specialized digital camera.  The camera's software (e.g BioRad ImageLab) quantifies the intensity of each band and allows us to compare the relative amounts of material (e.g. the fraction of DNA that has been cut at a specific location by an enzyme).  I've attached a sample.  \n","\n","Behold the sample!\n","\n","![Example image from Prof. Haushalter showing a gel with rectangles of various shades of grey. The figure is titled S326C hOGG1 Thermolability Assay. There is a bar chart at the bottom with time on the x-asis and Excision percent on the y-axis](https://kavassalis.space/s/Haushalter_Sample_gel_data.png)\n","\n","This is very exciting because the chemical 'signal' here is an image. While techniques for dealing with 2-dimensional data (like images!) look different in some respects from what we have been doing with 1-dimensional signals so far (spectra!), there are all sorts of things in common here.\n","\n","One thing critical to all kinds of data analysis is, first, understanding what the data represents. While a nuanced appreciation for this experiment would likely require you to take CHEM182 and 184, we'll do a speed read of a paper together and share out key takeaways before moving on with the notebook."],"metadata":{"id":"yJYBQWv1jaCE"}},{"cell_type":"markdown","source":["# A very quick primer on gel electrophoresis\n","\n","Gel electrophoresis is a way to separate mixtures of macromolecules (like DNA) based on their size (and other properties). The technique applies and electric field to the gel to cause the molecules to move through it. Smaller or more charged molecules move through the gel faster than larger or less charged molecules (and end up further down the gel).\n","\n","Here's my hand-wavy explanation as to how it's done (Prof. Haushalter can tell me how wrong I am!):\n","\n","1. Preparation of the Gel: The first step in gel electrophoresis is to prepare the gel, which is the medium through which the molecules will move. Depending on the type of macromolecule being separated, different types of gels may be used. Sometimes you'll see things like \"Polyacrylamide electrophoresis\", which tells you the gel used was Polyacrylamide.\n","\n","2. Loading of the Sample: The sample containing the mixture of molecules is loaded into wells in the gel.\n","\n","3. Application of an Electric Field: An electric field is applied once the sample is loaded. The macromolecules will start moving based on their size and charge. DNA and RNA carry a negative charge (I think?) and thus move toward the positive end of the field.\n","\n","4. Separation of the Molecules: As the molecules move through the gel, they will separate based on size and shape. Smaller molecules will move faster and travel further through the gel, while larger molecules will move slower and not travel as far.\n","\n","5. Visualization: After a certain period, the electric field is removed, and the separation of the molecules is visualized. This is often done using stains or dyes that bind to the macromolecules and can floresce under UV light (and be seen by a special camera!).\n","\n"],"metadata":{"id":"c_JGm4nLNjDs"}},{"cell_type":"markdown","source":["# Paper reading time!\n","\n","Open up the link to the class paper notes [Google Doc](https://docs.google.com/document/d/1BY2KcwaBKU78y2et_xvpmZCOYYMQ_a7avm7PFVJ_EDE/edit?usp=sharing).\n","\n","To ground yourself in what the paper is about, everyone should read the **Abstract** (it has two sections!), the **Introduction**, and the **Conclusions**. Use the shared doc to take notes (you can always add notes here too)."],"metadata":{"id":"jG9R4PEKkqED"}},{"cell_type":"markdown","source":["# Let's get into the computing\n","\n","Let's appreciate the elements of the above figure. The hefty piece of code below recreates it (some style adjustments could still be done). While only one \"plot\" is present (the bar graph at the bottom), `matplotlib` is happy to treat the three seperate pieces of information (the annotations with the experimental details), the photo of the gel, and the bar plot as three subplots. This style of multiplot is able to convey a lot of information, but all of the pieces are clearly related (which is different from some of the multiplots we looked at in the first class).\n","\n","Today, you don't need to spend much time going through the code for this figure. It is mostly here as an example for those who may pick final projects that require figures like these. After the break, you'll start deciding what you want to recreate!"],"metadata":{"id":"QlbEmGXgJ03A"}},{"cell_type":"code","source":["#Set up the main title and subplots\n","fig = plt.figure(figsize=(8, 8))  # adjust as needed\n","fig.suptitle('S326C hOGG1 Thermolability Assay', fontsize=36)\n","\n","# Create 2 subplots with different sizes\n","gs = gridspec.GridSpec(3, 1, height_ratios=[.25, 1, 3])  # adjust ratios to get the aesthetics right\n","\n","ax1 = plt.subplot(gs[0])   # subplot for annotations\n","ax2 = plt.subplot(gs[1])   # subplot for image\n","ax3 = plt.subplot(gs[2])   # subplot for bar graph\n","\n","# Keep your annotations and table in ax1\n","ax1.annotate('       37째C pre-incubation', (0.175, 0.85), xycoords='figure fraction', fontsize='16')\n","ax1.annotate('     _______________________', (0.175, 0.84), xycoords='figure fraction', fontsize='16')\n","ax1.annotate('        4째C pre-incubation', (0.58, 0.85), xycoords='figure fraction', fontsize='16')\n","ax1.annotate('     _______________________', (0.58, 0.84), xycoords='figure fraction', fontsize='16')\n","\n","ax1.annotate('Time (min):', xy=(0, -.2), xycoords='axes fraction', fontsize=14, ha='right')\n","\n","# You can then add the text above the first subplot in a table format\n","columns = [' ']*1 + [' ']*5 + [' ']*1 + [' ']*5\n","cell_text = [['0.5', '1', '5', '10', '30', '0',  '0.5', '1', '5', '10', '30']]\n","rows = [' ']\n","\n","# We don't actually want axis labels on this one because it's not a plot\n","ax1.axis('tight')\n","ax1.axis('off')\n","\n","table = ax1.table(cellText=cell_text, rowLabels=rows, colLabels=columns, cellLoc = 'center', rowLoc = 'center', loc='center')\n","table.auto_set_font_size(False)\n","table.set_fontsize(18)\n","table.scale(.95, 2.95)\n","for key, cell in table.get_celld().items():\n","    cell.set_linewidth(0)\n","\n","# Then you can add your gel electrophoresis example image\n","rect = patches.Rectangle((0,0),1,1,linewidth=2,edgecolor='black',facecolor='none', transform=ax2.transAxes)\n","ax2.add_patch(rect)\n","\n","# This loads in the image file\n","im = plt.imread('/content/gdrive/Shared drives/Chem_60_Spring_2024/In_Class_Notebooks/data/class14_test-electrophoresis.png')\n","ax2.imshow(im)\n","ax2.axis('off') # again, we don't want this to have axis\n","\n","# adding the annotation (this syntax makes text with an arrow pointing to what you want)\n","ax2.annotate('9-mer', xy=(0, .4), xytext=(-.05, .4),\n","            arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize = 12, xycoords='axes fraction', va='center', ha='right', textcoords='axes fraction')\n","\n","ax2.annotate('20-mer', xy=(0, .75), xytext=(-.05, .75),\n","            arrowprops=dict(facecolor='black', arrowstyle='->'), fontsize = 12, xycoords='axes fraction', va='center', ha='right', textcoords='axes fraction')\n","\n","# Plot for the bar plots in ax3\n","barWidth = 0.3\n","time = ['0.5', '1', '5', '10', '30']\n","\n","# from july 25 S326C thermo gel2 (where did this come from??)\n","bars1 = [0.05415693647*100, 0.03990872511*100, 0.1166135814*100, 0.1713672166*100, 0.3250584173*100]\n","bars2 = [0.2042089156*100, 0.1737426582*100, 0.3496066294*100, 0.3483463648*100, 0.4675014029*100]\n","\n","# The formatting of the bars can be changed for visual preference\n","r1 = np.arange(len(bars1)) + .1 # I am manually creating the positions of the bars\n","r2 = [x + barWidth + .1 for x in r1] # They should be a little bit apart\n","\n","# Now we add them to the plot\n","ax3.bar(r1, bars1, color='k', edgecolor='k', width=barWidth, label='37째C pre-incubation')\n","ax3.bar(r2, bars2, color='lightgray', edgecolor='k', width=barWidth, label=' 4째C pre-incubation')\n","\n","ax3.set_xlabel('Time (min)')\n","ax3.set_xticks([r + barWidth for r in range(len(bars1))], time)\n","ax3.set_ylabel('Excision (%)')\n","\n","# Setting the y-axis limits and ticks\n","ax3.set_ylim([0, 50])\n","ax3.set_yticks(range(0, 51, 10))\n","\n","# Remove frames\n","ax3.spines['right'].set_visible(False)\n","ax3.spines['top'].set_visible(False)\n","\n","plt.show()"],"metadata":{"id":"GqegqlrukqPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The data above is summarized with a bar chart. But the *data* actually comes from that gel image.\n","\n","**So how do the gels give us data?**\n","\n","The fluorescing molecules bound to the DNA (or other macromolecule), when exposed to a particular wavelength of light will absorb said light and then re-emit (fluoresce) at a longer wavelength. A special camera (known as a charge-coupled device) can take a picture of that emitted light. The brighter the light seen in a particular location, the more DNA is present at that location in the gel. These cameras can be set up to have have very long exposure times (and need to be fully shielded from any other light sources) to capture very small signals. It's very cool tech. The raw images go through some pretty sophisticated software to lead us to a dataset to tell our story."],"metadata":{"id":"Ex3oWV4TPlI6"}},{"cell_type":"markdown","source":["# A bit about working with images"],"metadata":{"id":"8Lo6H3PXoX_t"}},{"cell_type":"markdown","source":["I plotted a .png of the gel above, but let's look at the *raw* data here. Delightfully, these images are saved under a file type known as... GEL. `.gel` files are an image standard created to store gel electrophoresis data. They are an extention of a standard TIFF using private tags.\n","\n","That last part probably sounded like jargon (it is!). You likely have seen .tiff images before, but perhaps less often than .pngs. TIFF (Tagged Image File Format), like the PNG (Portable Network Graphics), is just an image file. There are differences in how the two file types store data (pixel values) for compression purposes, but for our needs, and why .gel files are based on .tiff files, the thing that makes the two special is what they store in *addition* to the pixel values (ie. they both come with bonus information, not just an image).\n","\n","The name \"Tagged Image File Format\" is alluding to this - \"tagged\" refers to the way information is stored and organized within the file. Each \"tag\" specifies the attributes of the image file, such as its dimensions, colour format, whether it is compressed or not, etc. You can also add custom tags to store whatever you want. This is how scientific meta data gets added to tiffs to create gels. Let's look at what I mean.\n","\n","First, we'll load an image."],"metadata":{"id":"kZK__DeZoYLk"}},{"cell_type":"code","source":["img = Image.open('/content/gdrive/Shared drives/Chem_60_Spring_2024/In_Class_Notebooks/data/class14_july_25_S326C_thermo_gel_2.gel')\n","img"],"metadata":{"id":"g9-DtWkToYid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oh no, an unprocessed image! We'll worry about what this is supposed to look like in a minute. Let's look at the metadata (the tags!)"],"metadata":{"id":"owh1KxXdsyEU"}},{"cell_type":"code","source":["metadata = img.tag_v2\n","print(metadata)"],"metadata":{"id":"nRhp8zVQpwTt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've got some numbers! The meta data comes to us as a Python dictionary. The \"keys\" of the dictionary (256, 257, etc.) are standardized tags that we can actually look up.\n","\n","- 256: ImageWidth. This indicates the width of the image, which is 886 pixels in this case.\n","- 257: ImageLength. This indicates the height (or length) of the image, which is 400 pixels in this case.\n","- 258: BitsPerSample. This indicates the bit depth of the image. (16,) means it is a 16-bit image.\n","- 259: Compression. The value of 1 typically stands for no compression. We don't typically want our raw data to be compressed.\n","- 262: PhotometricInterpretation. A value of 0 usually means WhiteIsZero.\n","- 269: DocumentName. This tag records the name of the document from which this image was created.\n","- 270: ImageDescription (this will be gel stuff!)\n","\n","Because this is a *standard*, you can look up any tag you want at the Library of Congress: https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml (you learned about other standards in CS5, like ASCII and UNICODE)."],"metadata":{"id":"W7fM1v5fs99U"}},{"cell_type":"markdown","source":["We can double check these properties if we want, like calling for the image width and height using `.size`."],"metadata":{"id":"hciQ-joWtq3k"}},{"cell_type":"code","source":["img.size"],"metadata":{"id":"rdRKtNKutH9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Important information is held here, not only for being able to interpret the gel, but for scientific reproducability! We can know when this file was made, where the data was originally stored, pretty much anything you would want to know.\n","\n","Because this is a Python dictionary, we can use the keys to grab details when we need them:"],"metadata":{"id":"l0OawcRbtJBV"}},{"cell_type":"code","source":["metadata[270]"],"metadata":{"id":"DaXr4uPtuDmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These were the settings the image was taken under."],"metadata":{"id":"pOUkES5fvmQ9"}},{"cell_type":"code","source":["metadata[305]"],"metadata":{"id":"t9Xh6IXWvp8N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the name and version of the camera software that was apparently running at the time (I can find very little about this particular version on the internet!). Things like this can be especially useful if it turns out there was some issue with a particular version of the software, and you need to track what samples were analyzed with said version.\n"],"metadata":{"id":"YB0FbKpBwAR2"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Try checking some of the other tags!"],"metadata":{"id":"VsAfptFvSv-a"}},{"cell_type":"code","source":["# see what else is stored."],"metadata":{"id":"ZwtLMsUbSveA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","To contrast this to a png, we can look at the already processed image that I used for the sample plot above:"],"metadata":{"id":"0oTMImLhSuxp"}},{"cell_type":"code","source":["img_png = Image.open('/content/gdrive/Shared drives/Chem_60_Spring_2024/In_Class_Notebooks/data/class14_test-electrophoresis.png')\n","img_png"],"metadata":{"id":"4Ia4i8TXp-mk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Try grabbing metadata for this. What happens?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"7i_r2DLPxIGF"}},{"cell_type":"code","source":["# try the syntax that worked on the .gel!"],"metadata":{"id":"N4Zl3nF2qIKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","The tags central to tiff files (or .gel files) give them a lot of flexibility in the kinds of information that they can pair with images. Because of this flexibility (storing all this extra information takes up space) they are often bulky files and aren't compatible with all devices (they're not great for the web or to use to share pictures if your goal is everyone opening and seeing the same picture).\n","\n","PNG, don't use tags, they use... 'chunks' (that's the official term for it). In a PNG, the data is broken into chunks which store all kinds of information, including what is functionally our metadata. Because pngs are designed to be compatible with all manner of image rendering devices, you can't embed custom information like in a tiff. The kinds of things stored are... harder to parse...\n","\n","Example:"],"metadata":{"id":"H4EkorhMqRWs"}},{"cell_type":"code","source":["img_png.info.items()"],"metadata":{"id":"ZJT7TYi7x_f-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The first \"chunk\" displayed in the above dictionary is the 'icc_profile'. This contains information on the International Color Consortium (ICC) profile used in the image. While it might be unreadable to me, it contains informatiion to help preserve colour accuracy across difference devices. Every device has its own way of rendering colours and the same image might look a bit different on two different screens (or printers). ICC profiles help to maintain colour consistency by transforming the colour data of the image using a \"profile connection space\". That is very useful for lots of applications, but not strictly the thing we need for scientific reproducability."],"metadata":{"id":"bEq5rsxFzJq0"}},{"cell_type":"markdown","source":["Okay. That's enough about images. Let's make the gel we loaded look like it was supposed to!"],"metadata":{"id":"Y0ukLd4voYzs"}},{"cell_type":"markdown","source":["# Why did our image look funny?\n","\n","Let's look at it again. Why isn't this what we expected (knowing what the png of this gel looks like)? We learned something from its metadata."],"metadata":{"id":"Hhk8d24CoY2f"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Resist scrolling for the answer and discuss with your neighbour why the image looks funny! What did its tags teach us? What do we remember about the colours associated with pixel values?\n","\n","\n","---\n","\n","\n"],"metadata":{"id":"3XkZh9vMTgbF"}},{"cell_type":"markdown","source":["**notes maybe?**"],"metadata":{"id":"o1JqBwhrTr-V"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Let's look at this again"],"metadata":{"id":"qaYaw7y-TuVH"}},{"cell_type":"code","source":["img"],"metadata":{"id":"tx4IUdVt1Agd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The PhotometricInterpretation tag with a value of 0 means that the image uses a \"WhiteIsZero\" colour scheme. In this scheme, a value of 0 (the minimum value for most image formats) represents white, while the maximum value (255 in the case of an 8-bit grayscale image which you probably saw in CS5 or 65535 for a 16-bit image!) represents black.\n","\n","The more common approach is \"BlackIsZero\" - that's certainly what Matplotlib is expecting. Here is a quick line plot showing what the colour '0' is normally:"],"metadata":{"id":"-De8w7U41By8"}},{"cell_type":"code","source":["plt.plot([0,1], color='0')"],"metadata":{"id":"5eXlZprB3MHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This should have made a black line.\n","\n","\n","\n","---\n","\n","\n","\n","Okay, so we first need to put things into the grayscale colour space our interpreter is expecting. Let's make the image a `numpy` array so we can work with it."],"metadata":{"id":"g_O1oUzM3Z22"}},{"cell_type":"code","source":["# Convert Image to Numpy array\n","im_array = np.array(img)\n","im_array"],"metadata":{"id":"8363-w4q2pLl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["16-bit grayscale stores a lot more information and nuiance between shades than 8-bit can. It makes my brain a bit unhappy though because these numbers are big... Because we are not going for maximum accuracy today, we will convert this into an 8-bit image (and then we can use a few more pre-built functions that happen to expect 0-255 to be our colour span).\n","\n","*If you want to know how to do this with a 16-bit image, this syntax will invert it (`img_inverted = img.point(lambda i: 65535 - i)`.*\n","\n","First, we're going to normalize our image.\n","\n","$$ x = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} $$\n","\n","This is technically not needed, but if you notice:"],"metadata":{"id":"WbH5PJI437Ek"}},{"cell_type":"code","source":["im_array.min(), im_array.max()"],"metadata":{"id":"cGnOdkga5Zqs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The minimum and maximum values of our gel aren't 0 and 65535, so we won't end up with true white or true black in the final image. We are essentially increasing the contrast of the gel to make it a little easier to both look at and work with (the difference between signal and background will be more apparent)."],"metadata":{"id":"B3z-blL85iPc"}},{"cell_type":"code","source":["# Normalize to 0-255\n","im_array_normalized = ((im_array - np.min(im_array)) * (1/(np.max(im_array) - np.min(im_array)) * 255)).astype('uint8')"],"metadata":{"id":"wtig7Y5r1MRU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check that that worked:"],"metadata":{"id":"Tx6n37sO6OO9"}},{"cell_type":"code","source":["# im_array_normalized.min(), im_array_normalized.max()"],"metadata":{"id":"ZGyLs1X36LCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","Did it work?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"sn7sQJPd6QK8"}},{"cell_type":"markdown","source":["Let's look at the new image:"],"metadata":{"id":"ulgYoFAr6aok"}},{"cell_type":"code","source":["# Convert numpy array back to an image.\n","img_normalized = Image.fromarray(im_array_normalized)\n","\n","# Invert colours (ImageOps.invert expects an 8-bit image)\n","img_inverted = ImageOps.invert(img_normalized)\n","img_inverted # this will just display the image so we can look at it"],"metadata":{"id":"NTSFQe-A6WiE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay. This at least looks right, if not... with some extra stuff we don't want. But we can crop the image to get rid of the extra stuff (what is the blob on the side outside the gel window? I do not know!)\n","\n"],"metadata":{"id":"AItbIJNt6lit"}},{"cell_type":"code","source":["#             left,upper,right,lower\n","crop_region = (240, 160, 750, 260)\n","cropped_img =  img_inverted.crop(crop_region)\n","cropped_img"],"metadata":{"id":"Tb7FlU4-669s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The variables within the `crop_region` tuple define a box by its left, upper, right, and lower pixel coordinates. The origin (0, 0) is in the upper-left corner. x-coordinates increase when you go right, and y-coordinates increase when you go down.\n","\n","So in the example:\n","\n","- `crop_region = (240, 150, 750, 260)`\n","\n","The specific values are as follows:\n","\n","- `240` is the **left** pixel coordinate. It is the x-coordinate of the left boundary of the box.\n","- `150` is the **upper** pixel coordinate. It is the y-coordinate of the top boundary of the box.\n","- `750` is the **right** pixel coordinate. It is the x-coordinate of the right boundary of the box.\n","- `260` is the **lower** pixel coordinate. It is the y-coordinate of the bottom boundary of the box.\n","\n","So, when `img_inverted.crop(crop_region)` was run, the image that should have been returned was a rectangular portion of the original image (with the colours inverted).\n","\n","Why did we have to do this manually? Surely an algorithm could have known what region we wanted?\n","\n","Honestly, no. There were several features in the regions we didn't want that looked just as 'salient' (visually important) as the features we did want. Image properties alone can't tell you where you actually would expect to see signal and where you wouldn't. Ideally, the camera software knowns where you have centred the sample though, and can crop with that knowledge."],"metadata":{"id":"dCk7INjr7ZUd"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","You can try recropping the image to get comfortable with the coordinates.\n","\n","I recommend returning to the example bounds (or close to them) before moving on though (if your crop includes weird stuff, the next stage of image processing is hard!)"],"metadata":{"id":"LkZpQnHGVVlr"}},{"cell_type":"markdown","source":["Now one final piece, because it so happens that blob on the side was very dark, our gel bands are kind of faint. I am going to renormalize the data (tweak the contrast again) so it looks a bit better."],"metadata":{"id":"uCwPKpFLG8Sd"}},{"cell_type":"code","source":["im_array = np.array(cropped_img)\n","im_array_normalized = ((im_array - np.min(im_array)) * (1/(np.max(im_array) - np.min(im_array)) * 255)).astype('uint8')\n","img_normalized_cropped = Image.fromarray(im_array_normalized)\n","img_normalized_cropped"],"metadata":{"id":"kd6RYssMG8ZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the object names I am using there. I am attempting to keep track of all the modifications I have made to the raw image. This is good practice (and easy to forget to do)."],"metadata":{"id":"kkudH9xGHM51"}},{"cell_type":"markdown","source":["# Let's extract information from the image"],"metadata":{"id":"YUT8mAm484aP"}},{"cell_type":"markdown","source":["Here is a biiig function. This is going to make a complex looking multiplot (that I am honestly delighted by). I recommend looking at how the function is called below and then coming up to read it. This isn't extracting information yet, just letting us look close up at what we have."],"metadata":{"id":"Lfuv70Nj_pgI"}},{"cell_type":"code","source":["def a_big_function_to_plot_the_gel_and_highlight_regions(gel_image, roi_top_left, width, height):\n","  # Compute the bottom-right coordinate\n","  roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","\n","  # 1. Create the grid for the plots\n","  fig = plt.figure(figsize=(15, 15))\n","  gs = gridspec.GridSpec(ncols=3, nrows=2, height_ratios=[3, 1])\n","\n","  # Define subplot axes\n","  ax0 = plt.subplot(gs[0, :2])  # Full gel image; spans first two columns\n","  ax1 = plt.subplot(gs[0, 2])  # ROI on top right\n","  ax2 = plt.subplot(gs[1, 2], projection='3d')  # 3D bar plot on bottom right\n","  ax3 = plt.subplot(gs[1, 0])  # X-Z view on bottom left\n","  ax4 = plt.subplot(gs[1, 1])  # Y-Z view on bottom middle\n","\n","  # 2. Highlight Region of Interest ROI in original image and plot\n","  drawn_image = gel_image.copy()\n","  cv2.rectangle(drawn_image, roi_top_left, roi_bottom_right, 0, 2)\n","  # ax0.imshow(drawn_image, cmap='gray')\n","  ax0.imshow(drawn_image, cmap='gray', vmin=0, vmax=255)\n","  ax0.set_title(\"Gel\")\n","  ax0.axis('off')\n","\n","  # 3. Extract pixels in the ROI and plot\n","  roi = gel_image[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","  # ax1.imshow(roi, cmap='gray')\n","  ax1.imshow(roi, cmap='gray', vmin=0, vmax=255)\n","  ax1.set_title(\"Region of Interest\")\n","  ax1.axis('off')\n","\n","  # 4. Create the X-Z view\n","  pixel_values = roi.flatten(); pixel_values_reshaped = np.reshape(pixel_values, (height, width))\n","  xz_view = np.mean(pixel_values_reshaped, axis=0) # (set axis=0 for mean along Y-axis)\n","  darkness_values = 255 - np.reshape(pixel_values, (height, width))\n","  dz = darkness_values.ravel()  # Flatten 'darkness' levels\n","  cmap = plt.cm.gray_r; darkness_xz = 255 - xz_view;\n","  norm = plt.Normalize(vmin=np.min(gel_image), vmax=np.max(gel_image))\n","  colour_xz = cmap(norm(darkness_xz))\n","  x_coords = np.arange(roi_top_left[0], roi_bottom_right[0])\n","  ax3.bar(x_coords, darkness_xz, color=colour_xz)\n","  ax3.set_xlabel('X')\n","  ax3.set_ylabel('Pixel Intensity')\n","  ax3.set_title('X-Z View')\n","\n","\n","  # 5. Create the Y-Z view\n","  yz_view = np.mean(pixel_values_reshaped, axis=1) # (set axis=1 for mean along X-axis)\n","  darkness_yz = 255 - yz_view\n","  norm = plt.Normalize(vmin=np.min(gel_image), vmax=np.max(gel_image))\n","  colour_yz = cmap(norm(darkness_yz))\n","  y_coords = np.arange(roi_top_left[1], roi_bottom_right[1])\n","  ax4.bar(y_coords, darkness_yz, color=colour_yz)\n","  ax4.set_xlabel('Y')\n","  ax4.set_ylabel('Pixel Intensity')\n","  ax4.set_title('Y-Z View')\n","\n","  # 6. Plot 3D view of pixel intensities in the ROI\n","  x_grid, y_grid = np.meshgrid(x_coords, y_coords)\n","  x = x_grid.ravel(); y = y_grid.ravel()\n","  # Compute 'darkness' levels for 3D representation\n","  z = np.zeros_like(dz)\n","  cmap = plt.cm.gray_r;\n","  norm = plt.Normalize(vmin=np.min(gel_image), vmax=np.max(gel_image))\n","  colours = cmap(norm(dz))\n","  bars = ax2.bar3d(x, y, z, 1, 1, dz, color=colours, shade=True)\n","  # ax2.set_zlabel('Pixel Intensity', fontsize=14)\n","  ax2.set_zlim([0,150])\n","  ax2.set_xlabel('x')\n","  ax2.set_ylabel('y')\n","  ax2.set_title('3D ROI')\n","\n","  # Adjust vertical space between plots\n","  plt.subplots_adjust(hspace=-0.5)\n","  # Adjust layout for labels visibility\n","  plt.tight_layout(pad=1.0)\n","  plt.show()"],"metadata":{"id":"pccnUnwp_pm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what this all does.\n","\n","We are creating another numpy array from the image so we can more easily do the needed operations in the figure function.\n","\n","Then, I am selecting one well (by visual inspection) that I want to look at."],"metadata":{"id":"BGEe9M_VAk0N"}},{"cell_type":"code","source":["gel_image = np.array(img_normalized_cropped)\n","\n","# Specify the initial top-left coordinates (x, y)\n","roi_top_left = (15, 5)\n","# Specify the width and height of rectangle\n","width, height = (50, 40)\n","\n","\n","a_big_function_to_plot_the_gel_and_highlight_regions(gel_image, roi_top_left, width, height)"],"metadata":{"id":"zFH_safvAlBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Talk with your neighbour - what is this showing? What kinds of information can we visually extract about the DNA in this particular well? What experimental conditions does it correspond to (look at the motivation figure for this).\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"-i-PIIvKCwxO"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"V7DqRl0lW6Y6"}},{"cell_type":"markdown","source":["\n","\n","---\n"],"metadata":{"id":"Y-D1mOmEW8L2"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Try to zoom in on another region in the figure and see what you see. What things are the same? What are different?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"i1KALOb7W_A4"}},{"cell_type":"code","source":["# some code! What parts of the above do you need to write down here? What from the above don't you need to write down here?"],"metadata":{"id":"F1OTftwqXLuw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**some notes**"],"metadata":{"id":"fezk_6G2XR63"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"za0QzsxKXTql"}},{"cell_type":"markdown","source":["# Background subtraction\n","\n","You likely noticed a range of gray around your well of choice. Despite our attempts to normalize things to give us a more clear background vs sample, we have a lot of gray. We need to perform some kind of background correction (remove the gray).\n","\n","While this will look quite different to the baseline correction we did for the IR spectra, the basic idea is the same. When you know what the background value should be, you can nudge it there (so long as you can identify a region you think should definitely be in the background).\n","\n","Methods for background correction usually involve subtracting an estimate of the background signal from the overall signal. The background signal can be estimated in several ways, such as by measuring the signal in an adjacent, empty area of the gel, or by assuming that the background signal is uniform across the gel. We'll do it in the simplest way possible here - find a region we label background, and subtract its average value everywhere.\n","\n","Let's look for one."],"metadata":{"id":"Gf5jX7wXC48-"}},{"cell_type":"code","source":["# Specify the initial top-left coordinates (x, y)\n","roi_top_left_bg = (230, 40)\n","# Specify the width and height of rectangle\n","width_bg, height_bg = (40, 40)\n","\n","a_big_function_to_plot_the_gel_and_highlight_regions(gel_image, roi_top_left_bg, width_bg, height_bg)"],"metadata":{"id":"DkknLO3dCw4-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","What properties would be required for a region to be considered an appropriate background? What does the region above represent?"],"metadata":{"id":"pV5ksS_9DetO"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"QKnZ4xSdYUzp"}},{"cell_type":"code","source":["# try finding a better region?"],"metadata":{"id":"mIaOOy2dDe6u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If things below break, come back to these background conditions:\n","\n","\n","```\n","# Specify the initial top-left coordinates (x, y)\n","roi_top_left_bg = (230, 40)\n","# Specify the width and height of rectangle\n","width_bg, height_bg = (40, 40)\n","\n","```\n","\n"],"metadata":{"id":"4fXDqmDHYZGE"}},{"cell_type":"markdown","source":["Let's confirm that we picked something for a background that looks distinct from a well we know we have a real signal in.\n","\n","A simple way to do that is with a histogram of the pixel values in both regions.\n","\n","Read through the below code to make sure what it is doing makes sense to you."],"metadata":{"id":"HRrnjtD5EiAO"}},{"cell_type":"code","source":["# roi for sample\n","roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","roi_sample = gel_image[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","# roi for background\n","roi_bottom_right_bg = (roi_top_left_bg[0] + width_bg, roi_top_left_bg[1] + height_bg)\n","roi_background = gel_image[roi_top_left_bg[1]:roi_bottom_right_bg[1], roi_top_left_bg[0]:roi_bottom_right_bg[0]]\n","\n","# flatten the ROIs (ie. make a 1d array not a 2d array)\n","pixels_sample = roi_sample.flatten()\n","pixels_background = roi_background.flatten()\n","\n","# create subplots\n","fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True, tight_layout=True)\n","\n","# plot histograms\n","axs[0].hist(pixels_sample, bins=256, color='blue', alpha=0.7)\n","axs[0].set_title('Sample')\n","\n","axs[1].hist(pixels_background, bins=256, color='red', alpha=0.7)\n","axs[1].set_title('Background')\n","\n","plt.show()"],"metadata":{"id":"8ZFTUOlkEiIF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTIE QUESTION\n","\n","How are these distributions the same? How are they different? Does looking at the distributions this way help you figure out what qualities you might want in a background region?\n","\n","\n","---\n","\n"],"metadata":{"id":"L2O4pk-TH1fG"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"AN0MlerzZKnq"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Now, let's come up with a background value to subtract. The simplest method is to just find the average background pixel value.\n"],"metadata":{"id":"Ijs-KiboZMJk"}},{"cell_type":"code","source":["# Compute the mean background intensity\n","mean_bg = 255 - np.mean(roi_background)\n","mean_bg"],"metadata":{"id":"lI21ikutH1mW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Why would I have taken 255 - the average? Is this a typo??\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"GBdKzrNmH-PV"}},{"cell_type":"markdown","source":["**notes for yourself**"],"metadata":{"id":"FKm9q0JwZr5A"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Here I am background 'subtracting' by ...\n","\n","\n","```\n","gel_image_background_corrected = gel_image+mean_bg\n","```\n","\n","\n"],"metadata":{"id":"adP5i-zPavq9"}},{"cell_type":"code","source":["gel_image_background_corrected = gel_image+mean_bg\n","a_big_function_to_plot_the_gel_and_highlight_regions(gel_image_background_corrected, roi_top_left, width, height)"],"metadata":{"id":"xSjdcfMnH-WH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","What did the above do? How does it look different from the previous plot?\n","\n","Could we have done this with different logic?\n","\n","\n","---\n","\n"],"metadata":{"id":"dhb2XGRliUje"}},{"cell_type":"markdown","source":["If you are going alter this process, make sure you look at the background_corrected background window too:"],"metadata":{"id":"Xwuf48vXipu8"}},{"cell_type":"code","source":["a_big_function_to_plot_the_gel_and_highlight_regions(gel_image_background_corrected, roi_top_left_bg, width_bg, height_bg)"],"metadata":{"id":"JTOvDlxuirbi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One thing to note, when we do background correction in this way (by floating point arithmetic), we end up creating an object that isn't a 8-bit image anymore."],"metadata":{"id":"Fh8Gilc9jBXf"}},{"cell_type":"code","source":["gel_image_background_corrected"],"metadata":{"id":"G-OFgSgLo4gl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["See? Instead of integer values between 0 and 255, we have floating point numbers (some greater than 255). While this will plot okay, we need to turn it back into an 8-bit image to do the next stage of image processing."],"metadata":{"id":"v4gak5vRpDXL"}},{"cell_type":"code","source":["background_corrected = ((gel_image_background_corrected - np.min(gel_image_background_corrected)) * (1/(np.max(gel_image_background_corrected) - np.min(gel_image_background_corrected)) * 255)).astype('uint8')"],"metadata":{"id":"4RtaQymskA2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["An alternative approach (that will actually keep the background looking more crisp, is to just clip the too high values: `background_corrected = np.clip(gel_image_background_corrected, 0, 255).astype('uint8')`. Important note in case you come back here to try this out on other images, just calling `gel_image_background_corrected.astype('uint8')` won't work as you might expect, because it will unhelpfully cast all numbers greater than 255 to their value minus 255 (so white pixels will become black).\n","\n"],"metadata":{"id":"a6qKFngvqRGY"}},{"cell_type":"markdown","source":["# Automate Finding the Bands\n","\n","So, we can locate the bands with our eyes, but surely we can just ask the computer to tells us where they are.\n","\n","We can, but it's imperfect. First, let's look at the pixel values in this image again."],"metadata":{"id":"WPMK2d3YcQgp"}},{"cell_type":"code","source":["# create plot\n","fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n","\n","# this needs to be 1D so we can make a histogram\n","pixels_sample = background_corrected.flatten()\n","# plot histograms\n","ax.hist(pixels_sample, bins=256, color='k', alpha=0.7)\n","# ax.set_ylim([0,600]) # it might help to change axis bounds to see features more clearly!\n","ax.set_title('Sample')\n","plt.show()"],"metadata":{"id":"_fNT7J69smOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do we see here? Shades of gray.\n","\n","If we want to have some very clear rule to tell the difference between sample and background, we'd  like to find some clear cut-off between the white space around the samples and the samples themselves. Unfortunately, this isn't easy because we see a continuum of values present. Excellent background subtraction can help, but the limitations of this kind of data means that we have to accept the cut-off won't always be clear.\n"],"metadata":{"id":"msDFxCi-tGUb"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Recreate the historgram, but zoom in on the y-axis (remember how `ax.set_ylim([a,b])` works) to see if you can identify something to differentiate sample from background in a way we might not have noticed yet.\n","\n","\n","---\n","\n"],"metadata":{"id":"LdGLctXuc2Wn"}},{"cell_type":"code","source":["# your new histogram"],"metadata":{"id":"uB54UlS-dLVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Perhaps you did see a bit of a bi-modal feature?"],"metadata":{"id":"T7BqET0FdNHU"}},{"cell_type":"markdown","source":["\n","Visual inspection of a histogram like this is a common way of 'thresholding'. Let's see what it means to divide the image into 'data' vs 'background' using the [`numpy.where()`](https://numpy.org/doc/stable/reference/generated/numpy.where.html) function.\n","\n","You've seen this function before, but let's go over the syntax because it's very powerful:\n","\n","`np.where(background_corrected <= 150, 255, 0)`\n","\n","This will return an object of the same size and shape as `background_corrected`. For every pixel in `background_corrected` that's value is <= 150 (the threshold I choose for this example), a value of 255 will be stored (white) and for every pixel with a value > 150, a value of 0 will be stored (black).\n","\n","Let's see it work:"],"metadata":{"id":"1jsorDS9cy26"}},{"cell_type":"code","source":["threshold = np.where(background_corrected <= 150, 255, 0).astype('uint8')\n","threshold"],"metadata":{"id":"uPgm5dTFsOMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It found some things! Not all the things though!\n","\n"],"metadata":{"id":"00YF25X9xCx_"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Change the threshold value above and see what it does. You may want to return it to 150 if the below breaks (but other values will work too and some will actually be better).\n","\n","\n","---\n","\n"],"metadata":{"id":"sac9N87mdvSQ"}},{"cell_type":"code","source":["# code!"],"metadata":{"id":"0yZRXuuHeBeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","At the end of the day, we want to know the pixel intensities (how much light was emitted) by each well. That means we need to know the location of each well. We have good guesses for the 20-mer thanks to the thresholding, but they're all different sizes and the edges aren't proper edges. There are several algorithms to solve this problem (but digging into them would take... too long for today. I recommend CS153 if this sparks joy). We are going to borrow a tool from the OpenCV library to just do this task  - it will find the rectangles that most closely match the regions in our binary image. This function is called [`cv2.findContours`](https://docs.opencv.org/4.x/d4/d73/tutorial_py_contours_begin.html). The implementation is below:\n"],"metadata":{"id":"T_AAgYHieCeJ"}},{"cell_type":"code","source":["# Find contours in the binary image\n","contours, hierarchy = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","# open cv will actually add these lines to the image, so we don't want it modifying our data\n","img_copy = copy.deepcopy(background_corrected)\n","\n","# Filter contours based on geometric properties\n","for contour in contours:\n","    # Get rectangle bounding contour\n","    [x,y,w,h] = cv2.boundingRect(contour)\n","\n","    # Discard small pieces that are less than 5% of the screen height (it will accidentally find tiny things we don't care about)\n","    if h < 0.05*background_corrected.shape[0]:\n","        continue\n","\n","    # Draw rectangle around contour and show them on the image\n","    cv2.rectangle(img_copy,(x,y),(x+w,y+h),(0,0,255),2)\n","\n","# Display marked image\n","plt.figure(figsize=(10,10))\n","plt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n","plt.show()"],"metadata":{"id":"cF6BDDWmjb5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If this looks very wrong, look at what is below and see if running all or some of this above your contour finder will make a difference:\n","\n","```\n","# Specify the initial top-left coordinates (x, y)\n","roi_top_left_bg = (230, 40)\n","# Specify the width and height of rectangle\n","width_bg, height_bg = (40, 40)\n","\n","# identify the background region for later use\n","roi_bottom_right_bg = (roi_top_left_bg[0] + width_bg, roi_top_left_bg[1] + height_bg)\n","roi_background = gel_image[roi_top_left_bg[1]:roi_bottom_right_bg[1], roi_top_left_bg[0]:roi_bottom_right_bg[0]]\n","\n","mean_bg = 255 - np.mean(roi_background)\n","gel_image_background_corrected = gel_image+mean_bg\n","background_corrected = ((gel_image_background_corrected - np.min(gel_image_background_corrected)) * (1/(np.max(gel_image_background_corrected) - np.min(gel_image_background_corrected)) * 255)).astype('uint8')\n","\n","threshold = np.where(background_corrected <= 150, 255, 0).astype('uint8')\n","\n","```\n"],"metadata":{"id":"xV4vcXgxyuy9"}},{"cell_type":"markdown","source":["## SHARE with the class\n","\n","If you changed any parameters above from the defaults I set (and I hope you did!), paste your image into the [same Google Doc](https://docs.google.com/document/d/1BY2KcwaBKU78y2et_xvpmZCOYYMQ_a7avm7PFVJ_EDE/edit?usp=sharing) we used at the start of class.\n","\n","Double check yours looks close enough to the others before going on."],"metadata":{"id":"qgBv-VE5yMC6"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","Now, let's split the 20-mer signals from the 9-mer. The 9-mer are more faint and were harder for our algorithm to find.\n","\n","The code looks at the relative position of the found contours (rectangles) and assigns them to the top or bottom row depending on where they appear in the image.\n","\n","\n","\n","```\n","    if y < background_corrected.shape[0] // 3:\n","      top_rectangles.append((x,y,w,h))\n","      cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,255),2) # highlight the top ones\n","    else: # if y >= y_threshold\n","        bottom_rectangles.append((x,y,w,h))\n","        cv2.rectangle(img_copy,(x,y),(x+w,y+h),(0,0,255),2)\n","```\n","\n"],"metadata":{"id":"5Qq23tkGfOv9"}},{"cell_type":"code","source":["top_rectangles = []\n","bottom_rectangles = []\n","\n","# do this on a fresh copy of the image\n","img_copy = copy.deepcopy(background_corrected)\n","\n","# Filter contours based on geometric properties\n","for contour in contours:\n","    # Get rectangle bounding contour\n","    [x,y,w,h] = cv2.boundingRect(contour)\n","\n","    # Discard small pieces that are less than 5% of the screen height (it will accidentally find tiny things we don't care about)\n","    if h < 0.05*background_corrected.shape[0]:\n","        continue\n","\n","    if y < background_corrected.shape[0] // 3:\n","      top_rectangles.append((x,y,w,h))\n","      cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,255),2) # highlight the top ones\n","    else: # if y >= y_threshold\n","        bottom_rectangles.append((x,y,w,h))\n","        cv2.rectangle(img_copy,(x,y),(x+w,y+h),(0,0,255),2)\n","\n","# Display marked image\n","plt.figure(figsize=(10,10))\n","plt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n","plt.show()\n","\n"],"metadata":{"id":"3rhJgmCkyu7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This should make the top ones all white and the bottom ones (however many found and it is okay so long as the number is > 1) black.\n","\n","Now, let's assume the rectangles should all be the same size. We want to compare equal areas, so we'll pick the largest and make them all the same.\n","\n","That is what is happening here:\n","\n","\n","\n","```\n","# Determine the size of the largest rectangle in top row\n","max_area_top = max(top_rectangles, key=lambda rect: rect[2]*rect[3])\n","    \n","# Adjust all the top rectangles to be centred around their original centre and have the size of the largest one\n","equalized_top_rectangles = [(x + w//2 - max_area_top[2]//2, y + h//2 - max_area_top[3]//2, max_area_top[2], max_area_top[3]) for x, y, w, h in top_rectangles]\n","\n","```\n","\n"],"metadata":{"id":"fzDd5AOPzkrb"}},{"cell_type":"code","source":["# Determine the size of the largest rectangle in top row\n","max_area_top = max(top_rectangles, key=lambda rect: rect[2]*rect[3])\n","\n","# Adjust all the top rectangles to be centred around their original centre and have the size of the largest one\n","equalized_top_rectangles = [(x + w//2 - max_area_top[2]//2, y + h//2 - max_area_top[3]//2, max_area_top[2], max_area_top[3]) for x, y, w, h in top_rectangles]\n","\n","# Plot rectangles\n","img_copy = copy.deepcopy(background_corrected)  # Create a copy of the image\n","for x,y,w,h in equalized_top_rectangles:\n","    cv2.rectangle(img_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n","\n","plt.figure(figsize = (10, 10))\n","plt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n","plt.show()"],"metadata":{"id":"GRUnGd430FxZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's look at the bottom ones. So long as we found one, we can use that to find out the distance we expect between the 20-mer and 9-mer positions:\n","\n","\n","```\n","# Determine the smallest y distance from the top rectangle to a bottom rectangle\n","min_y_distance = min([b[1] - t[1] for t in equalized_top_rectangles for b in bottom_rectangles if b[0] - t[0] < 5])\n","\n","```\n","\n","And then fill in the rest assuming the relative position is the same for the entire row:\n","\n","\n","\n","```\n","# Estimate the bottom rectangles\n","estimated_bottom_rectangles = [(x, y + min_y_distance, max_area_top[2], max_area_top[3]) for x,y,w,h in equalized_top_rectangles]\n","\n","```\n","\n","\n","\n"],"metadata":{"id":"fwRKNcTN2LAs"}},{"cell_type":"code","source":["# Determine the smallest y distance from the top rectangle to a bottom rectangle\n","min_y_distance = min([b[1] - t[1] for t in equalized_top_rectangles for b in bottom_rectangles if b[0] - t[0] < 5])\n","\n","# Estimate the bottom rectangles\n","estimated_bottom_rectangles = [(x, y + min_y_distance, max_area_top[2], max_area_top[3]) for x,y,w,h in equalized_top_rectangles]\n","\n","# Plot rectangles\n","img_copy = copy.deepcopy(background_corrected)  # Create a copy of the image\n","for x,y,w,h in estimated_bottom_rectangles:\n","    cv2.rectangle(img_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n","\n","plt.figure(figsize = (10, 10))\n","plt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\n","plt.show()"],"metadata":{"id":"LmNRQqOpzk0j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each of these rectangles is a region of interest that we want to compare!\n","\n","Let's go back and look at our function to plot the individual bands!\n","\n","Now, we don't have to manually enter the position of the samples, we can grab it from the list of lists we just made.\n","\n","Here is a 20-mer example."],"metadata":{"id":"LGKwwF-K2N6s"}},{"cell_type":"code","source":["roi_top_left = (equalized_top_rectangles[0][0], equalized_top_rectangles[0][1])\n","width = equalized_top_rectangles[0][2]\n","height = equalized_top_rectangles[0][3]\n","a_big_function_to_plot_the_gel_and_highlight_regions(gel_image_background_corrected, roi_top_left, width, height)"],"metadata":{"id":"ohyMA1IZ2dnn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And here is the corresponding 9-mer:"],"metadata":{"id":"IsJAO_b73dKy"}},{"cell_type":"code","source":["roi_top_left = (estimated_bottom_rectangles[0][0], estimated_bottom_rectangles[0][1])\n","width = estimated_bottom_rectangles[0][2]\n","height = estimated_bottom_rectangles[0][3]\n","a_big_function_to_plot_the_gel_and_highlight_regions(gel_image_background_corrected, roi_top_left, width, height)"],"metadata":{"id":"qz3NsSE13dRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","What experimental conditions do these correspond to?\n","\n","\n","---\n","\n"],"metadata":{"id":"4H45x1mF38FS"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"4-DbPiQsg4Tk"}},{"cell_type":"markdown","source":["\n","\n","---\n","Okay, if you made it here, that's great! You can submit your notebook now, or... continue below...\n"],"metadata":{"id":"HPse4ROI4W9j"}},{"cell_type":"markdown","source":["# Submit your notebook\n","\n","It's time to download your notebook and submit it on Canvas. Go to the File menu and click **Download** -> **Download .ipynb**\n","\n","Then, go to **Canvas** and **submit your assignment** on the assignment page. Once it is submitted, swing over to the homework now and start working through the paper."],"metadata":{"id":"y5rNn97ftVIO"}},{"cell_type":"markdown","source":["# Integration\n","\n","How do we find the signal strength corresponding to each well? We need to numerically integrate. This one is just adding up the pixel values (in a way that makes sense for what 0 and 255 represent)."],"metadata":{"id":"9YlNoWBlhG0P"}},{"cell_type":"code","source":["# reminder of how to grab the coordinates for a region of interest\n","roi_top_left = (estimated_bottom_rectangles[0][0], estimated_bottom_rectangles[0][1])\n","width = estimated_bottom_rectangles[0][2]\n","height = estimated_bottom_rectangles[0][3]\n","roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","\n","# and define the ROI itself\n","roi = gel_image_background_corrected[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n"],"metadata":{"id":"-d4cY_Lt4XIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Write a function to calculate the pixel intensities (integrate) for a given ROI. The function should return a large value when the ROI emitted a lot of light (ie. it looked dark!).\n","\n","```\n","def integrate_roi(roi):\n","  # some things!\n","  return integrand\n","```\n","\n"],"metadata":{"id":"7ag4W_DZ5fM7"}},{"cell_type":"code","source":["def integrate_roi(roi):\n","\n","  return integrand"],"metadata":{"id":"jWFhdJNK5fUI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test it out!"],"metadata":{"id":"_MgHAkFVh_d0"}},{"cell_type":"code","source":["roi_top_left = (equalized_top_rectangles[0][0], equalized_top_rectangles[0][1])\n","width = equalized_top_rectangles[0][2]\n","height = equalized_top_rectangles[0][3]\n","roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","roi = gel_image_background_corrected[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","\n","integrate_roi(roi)"],"metadata":{"id":"BRUFgq3Y52o0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["vs"],"metadata":{"id":"OiO04RIu6DdW"}},{"cell_type":"code","source":["roi_top_left = (estimated_bottom_rectangles[0][0], estimated_bottom_rectangles[0][1])\n","width = estimated_bottom_rectangles[0][2]\n","height = estimated_bottom_rectangles[0][3]\n","roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","roi = gel_image_background_corrected[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","\n","integrate_roi(roi)"],"metadata":{"id":"Ijy6UxTD6D3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's the ratio between these values that generates the numbers in the bar plot!\n","\n","If your function to find the integral works, run the below code and see what you get..."],"metadata":{"id":"wzoSEE4A6M3R"}},{"cell_type":"code","source":["list_to_store_results=[]\n","\n","for experiment in range(len(equalized_top_rectangles)):\n","  roi_top_left = (equalized_top_rectangles[experiment][0], equalized_top_rectangles[experiment][1])\n","  width = equalized_top_rectangles[experiment][2]\n","  height = equalized_top_rectangles[experiment][3]\n","  roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","  roi = gel_image_background_corrected[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","\n","  top_integral = integrate_roi(roi)\n","\n","  roi_top_left = (estimated_bottom_rectangles[experiment][0], estimated_bottom_rectangles[experiment][1])\n","  width = estimated_bottom_rectangles[experiment][2]\n","  height = estimated_bottom_rectangles[experiment][3]\n","  roi_bottom_right = (roi_top_left[0] + width, roi_top_left[1] + height)\n","  roi = gel_image_background_corrected[roi_top_left[1]:roi_bottom_right[1], roi_top_left[0]:roi_bottom_right[0]]\n","\n","  bottom_integral = integrate_roi(roi)\n","\n","  list_to_store_results.append([equalized_top_rectangles[experiment][0],bottom_integral/top_integral])\n","list_to_store_results.sort()"],"metadata":{"id":"Gh6ynKhG7ByA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the plot"],"metadata":{"id":"vWXdIfsOiaqe"}},{"cell_type":"code","source":["fig, ax3 = plt.subplots(1,1)\n","# Plot for the bar plots in ax3\n","barWidth = 0.3\n","time = ['0.5', '1', '5', '10', '30']\n","\n","# from july 25 S326C thermo gel2 (Here they are)\n","bars1 = [results[1]*100 for results in list_to_store_results[:5]]\n","bars2 = [results[1]*100 for results in list_to_store_results[6:]]\n","\n","\n","# The formatting of the bars can be changed for visual preference\n","r1 = np.arange(len(bars1)) + .1 # I am manually creating the positions of the bars\n","r2 = [x + barWidth + .1 for x in r1] # They should be a little bit apart\n","\n","# Now we add them to the plot\n","ax3.bar(r1, bars1, color='k', edgecolor='k', width=barWidth, label='37째C pre-incubation')\n","ax3.bar(r2, bars2, color='lightgray', edgecolor='k', width=barWidth, label=' 4째C pre-incubation')\n","\n","ax3.set_xlabel('Time (min)')\n","ax3.set_xticks([r + barWidth for r in range(len(bars1))], time)\n","ax3.set_ylabel('Excision (%)')\n","\n","# Setting the y-axis limits and ticks\n","# ax3.set_ylim([0, 50])\n","# ax3.set_yticks(range(0, 51, 10))\n","\n","# Remove frames\n","ax3.spines['right'].set_visible(False)\n","ax3.spines['top'].set_visible(False)"],"metadata":{"id":"Kmhcx6oj75-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","[Share](https://docs.google.com/document/d/1BY2KcwaBKU78y2et_xvpmZCOYYMQ_a7avm7PFVJ_EDE/edit?usp=sharing) this plot too!"],"metadata":{"id":"er_6YBplib6c"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"SvE2DJP950R2"}},{"cell_type":"markdown","source":["# Unneeded extra fun with K-means\n","\n","Instead of the thresholding we did above, we could have actually done it with K-means clustering!\n","\n","This function is almost the same as the last time we wrote it, but for an entirely different problem and data type"],"metadata":{"id":"ucnLho00ijTZ"}},{"cell_type":"code","source":["def kmeans(image, K, max_iters=100):\n","    # Flatten the image to 1D array\n","    pixels = image.flatten()\n","\n","    # Step 1: Initialize random centroids\n","    centroids = np.random.choice(pixels, K)\n","\n","    for _ in range(max_iters):\n","        # Step 2: Form clusters\n","        clusters = np.argmin(np.abs(pixels[:, None] - centroids), axis=1)\n","\n","        new_centroids = np.array([pixels[clusters==k].mean() for k in range(K)])\n","\n","        # If centroids don't change significantly, break\n","        if np.allclose(centroids, new_centroids):\n","            break\n","\n","        centroids = new_centroids\n","\n","    # Replace each pixel value with the corresponding centroid\n","    segmented_image = centroids[clusters].reshape(image.shape).astype(np.uint8)\n","\n","    return segmented_image"],"metadata":{"id":"7EsMNrjDitMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see"],"metadata":{"id":"JzwPsaHXivaB"}},{"cell_type":"code","source":["segmented_image = kmeans(gel_image, K=2)\n","segmented_image"],"metadata":{"id":"elrfs2FLiwSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How awesome is that.\n","\n","This is just to show the amazing generality of some of the techniques you are learning!"],"metadata":{"id":"8-n_wj2xiy0Y"}}]}