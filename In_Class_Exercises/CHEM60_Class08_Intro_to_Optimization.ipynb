{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CHEM 60 - February 14th, 2024 (Intro to Optimization)\n","\n","We are going to wind our way to the notion of optimization today. Optimization problems come up pretty much everywhere in chemistry (and other fields) and can take many forms. We actually did one type of optimization last week (the brute-force curve fitting example was an optimization). When doing your Molecular Monday calculations, a lot of optimization has gone on behind the scenes.\n","\n","From optimizing conditions in chemical synthesis to optimizing molecular geometry in a DFT calculation, there is  shared logic and mathematics that is really worth understanding.\n","\n","When we optimize *something*, that means we are searching for some maximum or minimum value - a maximum yield, a maximum $r$-coeffient, a minimum energy, minimum residuals, etc. In order to 'optimize' our *something* (target, let's say), we need to have variables or parameters we can, well, vary that will impact said target.\n","\n","In the example with curve fitting last week, our target was the sum of the squared residuals, and we wanted to minimize it (\"least squares\"). Our variables - or parameter space - consisted of a range of possible slopes and intercepts.\n","\n","\n","To get started, click on '**File**' in the left menu, then '**Save a copy in Drive**' to ensure you are editing *your* version of this assignment (if you don't, your changes won't be saved!). After you click '**Save a copy in Drive**' a popup that says **Notebook copy complete** should appear, and it may ask you to <font color='blue'>**Open in a new tab**</font>. When open, your new file will be named `Copy of CHEM60_Class_8... .ipynb` (you may want to rename it before/after you move it to your chosen directory)."],"metadata":{"id":"BgQiJBZhVelD"}},{"cell_type":"markdown","source":["# Imports\n","\n","Here are the Python imports that we will need today. A couple of new ones are included, along with some extra formatting (a custom colour map we might use).\n","\n","Run the below code block to get started."],"metadata":{"id":"JkJEvAWu7Qfx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dmfx0lxMvOn3"},"outputs":[],"source":["# Third party imports\n","import matplotlib.colors as colours  # Library for defining colours for plotting\n","import matplotlib.pyplot as plt  # Library for creating plots and visualizations\n","import numpy as np  # Library for numerical computing\n","import pandas as pd  # Library for data manipulation and analysis\n","import plotly.graph_objects as go  # Library for creating interactive plots\n","from statsmodels.stats.multicomp import pairwise_tukeyhsd # a stats test we'll use\n","from scipy import linalg  # Library for linear algebra operations\n","from scipy.stats import distributions # Module that loads in pre-made statistical distributions\n","from scipy.stats import f # Module for F statistics to get P-values\n","from scipy.interpolate import griddata  # Module for data interpolation on a grid\n","\n","# This part of the code block is telling matplotlib to make certain font sizes exra, extra large by default\n","# Here is where I list what parametres I want to set new defaults for\n","params = {'legend.fontsize': 'xx-large',\n","         'axes.labelsize': 'xx-large',\n","         'axes.titlesize':'xx-large',\n","         'xtick.labelsize':'xx-large',\n","         'ytick.labelsize':'xx-large'}\n","plt.rcParams.update(params)\n","\n","# below sets up some custom colour maps for certain visualizations\n","# colour blind friendly colour map (blue )\n","cmap_hex_temp = ['#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c']\n","cmap_RBG_temp = [colours.to_rgb(cmap_hex_temp[i]) for i in range(len(cmap_hex_temp))]\n","cmap_temp = colours.LinearSegmentedColormap.from_list('cmap',cmap_RBG_temp,N=256)\n","# Calculate color level values for each color\n","colour_levels = [i/float(len(cmap_hex_temp)-1) for i in range(len(cmap_hex_temp))]\n","# Create the colorscale list for Plotly\n","colourscale = [[colour_levels[i], cmap_hex_temp[i]] for i in range(len(cmap_hex_temp))]"]},{"cell_type":"markdown","source":["On Monday, you saw a nice example of finding the distance associated with the minimum energy for the O-O system (ie. $\\text{O}_2$). This is an example of geometry optimization."],"metadata":{"id":"KUxCq6rmcfO0"}},{"cell_type":"code","source":["# we're not re-running these day!\n","dist = [0.5       , 0.5862069 , 0.67241379, 0.75862069, 0.84482759,  0.93103448, 1.01724138, 1.10344828, 1.18965517, 1.27586207,1.36206897, 1.44827586, 1.53448276, 1.62068966, 1.70689655,1.79310345, 1.87931034, 1.96551724, 2.05172414, 2.13793103,2.22413793, 2.31034483, 2.39655172, 2.48275862, 2.56896552,2.65517241, 2.74137931, 2.82758621, 2.9137931 , 3.        ]\n","energy = [-142.83145598888603,-145.75368546778373, -147.48581182092795, -148.48178800434505, -149.03081058146063, -149.32484884392164,-149.4693374365733, -149.52713923490964, -149.53543166711282, -149.51619309250424, -149.4826317854426,-149.4422527260286, -149.39936341364097, -149.3559334484894, -149.31439704290997, -149.2750698381664,-149.23838283368121, -149.20411880460887, -149.1730433537303, -149.1446194036704, -149.11867257567388,-149.09501513175158, -149.07322402577395, -149.05362473403744, -149.03579277603419, -149.01957653617893,-149.00483321293848, -148.9912790073016, -148.97909612853542, -148.96800065923728]\n","\n","# here is the plot\n","plt.figure(figsize=(6,5))\n","plt.plot(dist,energy)\n","plt.xlabel('distance (Å)')\n","plt.ylabel('energy ($E_h$)')\n","\n","# Finding the minimum distance and its corresponding energy value\n","d_min = dist[np.argmin(energy)]\n","\n","# Annotating the minimum distance on the plot\n","plt.annotate('Minimum Distance = '+str(round(d_min,3))+\" Å\", xy=(d_min, np.min(energy)), xytext=(d_min, np.min(energy) + 1),\n","             arrowprops=dict(facecolor='black', arrowstyle='->'))\n","plt.title(\"$O_2$ Energy vs O-O Distance \\n psi4, scf/6-31++G(d,p)\")\n","plt.show()"],"metadata":{"id":"ftNDp3CXS7Wx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, the target was energy (and we wanted a minimum), and the parameter space consisted of only one variable - distance.\n","\n","All manner of chemists think about optimization problems. One that we'll look at today might seem quite different from the above, but it is the same fundamental operation: Reaction Optimization.\n","\n","When designing a new chemical synthesis, how do you know how much reagent or catalyst to use? How do you understand the optimal reaction conditions? You vary them, just like the distance was varied above."],"metadata":{"id":"ujin3tZFWVk_"}},{"cell_type":"markdown","source":["# Reaction Optimization\n","\n","Jonathan and Matthew picked a paper for us in our first class together that was all about optimization!\n","\n","Glavinović et al.'s \"[A chlorine-free protocol for processing germanium](https://www.science.org/doi/10.1126/sciadv.1700149)\". You'll spend some time with it today and in your homework. (a pdf of the paper and its supplementary material are found in [In_Class_Notebooks](https://drive.google.com/drive/folders/1hqeix2IYjN9PxOK2pn63Y4gucbuMONVr?usp=share_link) directory in the Drive, too).\n","\n","Here is the abstract to get you thinking about its about and why its important:\n",">Replacing molecular chlorine and hydrochloric acid with less energy- and risk-intensive reagents would markedly\n","improve the environmental impact of metal manufacturing at a time when demand for metals is rapidly increasing.\n","We describe a recyclable quinone/catechol redox platform that provides an innovative replacement for elemental\n","chlorine and hydrochloric acid in the conversion of either germanium metal or germaniumdioxide to a germanium\n","tetrachloride substitute. Germanium is classified as a “critical” element based on its high dispersion in the\n","environment, growing demand, and lack of suitable substitutes. Our approach replaces the oxidizing capacity of chlorine with molecular oxygen and replaces germanium tetrachloride with an air- and moisture-stable Ge(IV)-\n","catecholate that is kinetically competent for conversion to high-purity germanes.\n","\n","\n","So, traditional Germanium processing has some big problems from a Green Chemistry lens. The authors present a new synthesis - that they claim is generalizable beyond germanium - to process this metal more safely and efficiently. Sounds good. So, how do you systematically optimize a reaction?  \n","\n","One way to do it is by varying individual components of the reaction and seeing what happens (that's what the authors of this paper did).\n","![Glavinović et al., supplementary figure 1 showing reaction scheme](https://kavassalis.space/s/Glavinovic_2017_S1_Optimization_of_LAG_additive_composition.png)\n","\n","\n","We'll step through their process so you can see what this looks like. This reaction involves griding up the reactants - they use something called liquid-assisted\n","grinding (LAG) to do so. What liquid did they pick? And how did they pick it?\n","\n","In the supplement, the authors explain their process.\n","\n","> The mechanical milling conditions were optimized to promote full conversion of starting materials into either crystalline phases 3 or 3a. By qualitative evaluation of the PXRD patterns of crude reaction mixtures, the LAG additive composition, LAG additive volume, Py equivalents, milling time and milling frequency were optimized so that crystalline phases corresponding to starting materials were not detected, and crystalline phases corresponding to 3 or 3a were dominant.\n","\n","A key thing to note here is \"qualitative evaluation\" of the PXRD. PXRD, or Powder X-ray Diffraction, is a technique used to study crystalline materials. It operates by directing X-rays at a sample, which then diffract in various directions. By studying the angles and intensities of these diffracted beams, chemists can gain detailed information about the structure and chemical composition of the material."],"metadata":{"id":"CB7y3rOEeQFs"}},{"cell_type":"markdown","source":["# Optimization of LAG additive composition.\n","\n","Check out table S1 in the supplement (make sure I didn't make any mistakes!)\n","\n","Does looking at this table tell you how/why 1:1 PhMe:H₂O ended up being the winning LAG liquid?"],"metadata":{"id":"81dGuldQb3QY"}},{"cell_type":"code","source":["# Define the DataFrame columns\n","columns = ['LAG Liquid', 'LAG Volume (μL)', 'Pyridine (equiv)', 'Milling Time (mins)', 'Milling frequency (Hz)', 'Scale (mg)', 'Observed SM Phase', 'Observed Product Phase']\n","\n","# Initialize an empty DataFrame\n","LAG_liquid_df = pd.DataFrame(columns=columns)\n","\n","# Fill in the DataFrame with your data\n","LAG_liquid_df.loc[0] = ['Neat', 60, 2, 90, 25, 200, 'Ge. 1', '3'] # values for Entry (1)\n","LAG_liquid_df.loc[1] = ['H₂O', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (2)\n","LAG_liquid_df.loc[2] = ['MeOH', 60, 2, 90, 25, 200, 'Ge. 1', '3a'] # values for Entry (3)\n","LAG_liquid_df.loc[3] = ['PhOH', 60, 2, 90, 25, 200, 'Ge. 1', '3'] # values for Entry (4)\n","LAG_liquid_df.loc[4] = ['1:1 PhMe:H₂O', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (5)\n","LAG_liquid_df.loc[5] = ['1:1 PhMe:MeOH', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (6)\n","\n","# now let's call the dataframe to look at the table\n","LAG_liquid_df"],"metadata":{"id":"GuOMBfYax1yo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The authors state:\n","> The resulting green/brown crude solids were analyzed by PXRD (fig . S2). The optimal liquid additive composition was determined to be 1:1 PhMe : H2O (entry 5)\n","\n","Okay - how did they know? Looking at the PXRD data."],"metadata":{"id":"zm8kyxU30Ewl"}},{"cell_type":"markdown","source":["They didn't provide their raw PXRD data, so... I have made some up! This part - the making some-up part - is not part of the reaction optimization process. But it is useful to demonstrate the optimization techniques we want to work through.\n","\n","If you look at the PXRD data in the paper, you'll see peaks that look kind-of Gaussian associated with some funny \"2 theta\" angles. In PXRD, the peaks in the \"diffractogram\" (the graph resulting from the X-ray analysis) represent the response from the sample when it is struck by the X-rays. Each peak corresponds to a specific plane of atoms within the crystal structure of the material. The 2 theta (2Θ) angles are the angles at which these peaks occur. ie. 2Θ is the diffracted angle at which X-rays scatter off of the planes in the crystalline sample. The specific angles of diffraction (and the positions of the peaks) are unique to the atomic arrangement of the sample analyzed. This allows the authors to identify the structures of unknown samples by comparing the 2Θ values to known diffraction patterns.\n","\n","You don't need to know the details of how PXRD works, but the big things to know are that the peak heights (intensities) and 2Θ angles let you know what you are looking at chemically.\n","\n","Because we know we expect some Gaussian-y peaks at specific 2Θs, plus some noise, I wrote a quick function to simulate their data."],"metadata":{"id":"HF3pcSkePzxM"}},{"cell_type":"code","source":["# Function to generate a Gaussian peak\n","def gaussian(x, height, centre, width):\n","    return height * np.exp(-(x - centre) ** 2 / (2 * width ** 2))"],"metadata":{"id":"xmmvVKzfPz7y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Test the above function to see what it gives you so you know *how* the synthetic data is being generated.\n","\n","What type of object is it spitting out? What do the different terms mean?\n","\n","\n","---\n","\n"],"metadata":{"id":"EiW8gESsdOr4"}},{"cell_type":"code","source":["# Create a range of 2-theta values\n","theta = np.linspace(4, 40, 1000)\n","### try some different values before\n","height = 0\n","centre = 0\n","width = 0\n","# testing the thing!\n","gaussian(theta, height, centre, width)"],"metadata":{"id":"izLWuWd9dWWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","Okay, if you look at Fig. S2 from the author', you'll notice many peaks. I eye-balled the ones that looked important and created a list of fake peaks below. This part is distinctly different from what the authors did (we assume!)."],"metadata":{"id":"NFvNtU0zP1Td"}},{"cell_type":"code","source":["# Define parameters for the Gaussian peaks for each PXRD pattern: height, mean, width for each peak\n","all_params = [\n","    { 'params': [(1, 21, .1), (3, 25.5, .1), (15, 27, .1)], 'name': 'Ge', 'colour': '#377eb8', 'noise':.2},\n","    { 'params': [(1, 7, .1), (1, 12.2, .2), (15, 13, .1), (1, 17, .2), (1.5, 19, .2), (1, 22, .2), (1, 25, .2), (1, 30, .2)], 'name': '$C_{14}H_{20}O_2$', 'colour': '#e41a1c', 'noise':.3},\n","    { 'params': [(1, 8.5, .1), (1, 12.2, .2), (15, 13, .1), (3, 17, .2), (3.5, 19, .2), (1, 22, .2), (2, 25, .2), (1, 25.5, .1), (10, 27, .1), (1, 30, .2)], 'name': 'Neat', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (1, 12.2, .2), (15, 13, .1), (3, 17, .2), (3.5, 19, .2), (1, 22, .2), (2, 25, .2), (1, 25.5, .1), (10, 27, .1), (1, 30, .2)], 'name': 'H₂O', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 9.5, .1), (2, 12.2, .2), (6, 13, .1), (3, 17, .2), (3.5, 19, .2), (1, 22, .2), (2, 25, .2), (1, 25.5, .1), (5, 27, .1), (1, 30, .2)], 'name': 'MeOH', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(6, 8.5, .1), (2, 12.2, .2), (15, 13, .1), (3, 17, .2), (3.5, 19, .2), (1, 22, .2), (2, 25, .2), (1, 25.5, .1), (6, 27, .1), (1, 30, .2)], 'name': 'PhMe', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': '1:1 PhMe:H₂O', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(5, 8.5, .1), (1, 12.2, .2), (15, 13, .1), (5, 17, .2), (6.5, 19, .2), (1, 22, .2), (2, 25, .2), (1, 25.5, .1), (10, 27, .1), (1, 30, .2)], 'name': '1:1 PhMe:MeOH', 'colour': '#4daf4a', 'noise':.5},\n","]\n"],"metadata":{"id":"-gY01bkrP2OK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's look at what this."],"metadata":{"id":"xTFK-tTjeFxp"}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10, 10))\n","theta = np.linspace(4, 40, 1000)\n","\n","# Generate and plot PXRD patterns for each parameter set\n","for i, param_dict in enumerate(all_params):\n","\n","    # Generate noise and add it to the y-data\n","    np.random.seed(0)  # For reproducibility\n","    noise = np.random.normal(0, param_dict['noise'], theta.shape)\n","\n","    # Generate the y-data for the PXRD pattern\n","    y = sum(gaussian(theta, height, mean, width) for height, mean, width in param_dict['params'])\n","    y += noise\n","\n","    # Add the offset for easier visualization\n","    y -= i * 20\n","\n","    # Plot the PXRD pattern\n","    ax.plot(theta, y, lw=3, color=param_dict['colour'])\n","\n","    # Create a text label for the PXRD pattern\n","    ax.text(42, -i*20, param_dict['name'], verticalalignment='center', color=param_dict['colour'])\n","\n","# Set the x-ticks to start at 4 and increment by 5\n","ax.set_xticks(range(4, 40, 5))\n","\n","# Remove y-axis ticks\n","ax.yaxis.set_ticklabels([])\n","\n","# Hide grid lines\n","ax.grid(False)\n","\n","# Set the x-axis label\n","ax.set_xlabel('$2 \\\\theta$')\n","\n","plt.show()"],"metadata":{"id":"HPpIDjmjk99o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look! Fake data! It looks pretty decent. Okay, how did the above figure let them pick the optimal LAG liquid?"],"metadata":{"id":"a8YEC_3ccgF4"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Talk with your neighbours - why did the above let the authors pick the ideal LAG liquid? They did this step qualitatively - how would the above let you do that?\n","\n","\n","---\n","\n"],"metadata":{"id":"BdlfSHX_e2zk"}},{"cell_type":"markdown","source":["**you can write notes if that will help you**"],"metadata":{"id":"698cS43AfN5C"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"3NSYoLaAfMvw"}},{"cell_type":"markdown","source":["# Which is best?\n","\n","What if we wanted to quantify which one was better? First, we'd need to have defined the notion of 'better' (what you discussed above, presumably). Then, we'd want to know if - statistically - the different liquids had significantly different performance.\n","\n","To do this, we'd need repeated measurements (ie. not just one PXRD spectra for each liquid, but several so we could say if the differences were meaningfully consistent).\n","\n","Let's functionalize our fake data maker to make fake replicates now, too."],"metadata":{"id":"QqjZKKn7fW68"}},{"cell_type":"code","source":["# Create a function to generate Gaussian peaks and return the y-values\n","def generate_pattern(theta, params, noise_level):\n","    noise = np.random.normal(0, noise_level, theta.shape)\n","    y = sum(gaussian(theta, height, mean, width) for height, mean, width in params)\n","    y += noise\n","    return y"],"metadata":{"id":"HlsjlqXBfaAz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, instead of looking at the entire spectra, let's only concern ourselves with the two regions we are interested in (the regions of the spectra that let us know that some of our reactants ended up in our final product).\n","\n","We aren't doing fancy peak finding or integrating under curves yet (soon, though!). Instead, we'll just pick the peak values associated with the two peaks we know to be significant (because our two main reactants have distinct, characteristic theta values)."],"metadata":{"id":"99dY4GA7kcbP"}},{"cell_type":"code","source":["# Initializing an empty list to store each dataframe\n","df_list = []\n","\n","# Iterate through each entry in all_params\n","for i, entry in enumerate(all_params[2:]):\n","    # Repeat three times for replicates\n","    for j in range(3):\n","        y_vals = generate_pattern(theta, entry['params'], entry['noise'])\n","        # 2 theta locations of the Peaks\n","        peak_I_location = 13\n","        peak_Ge_location = 27\n","        # Find the closest theta value to the peak location & get corresponding y value from y_vals\n","        peak_I_value = y_vals[(np.abs(theta - peak_I_location)).argmin()]\n","        peak_Ge_value = y_vals[(np.abs(theta - peak_Ge_location)).argmin()]\n","        # Create a new dataframe for each peak value and append it to df_list\n","        new_df = pd.DataFrame({'LAG Liquid': [entry['name']], '1 Peak': [peak_I_value], 'Ge Peak': [peak_Ge_value], 'Replicate': [j]})\n","        df_list.append(new_df)\n","\n","# Combine all the dataframes in df_list\n","peak_values = pd.concat(df_list, ignore_index=True)"],"metadata":{"id":"v_VQQJ5QAYNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at what we've made now:"],"metadata":{"id":"uB9GLv7ikv3P"}},{"cell_type":"code","source":["peak_values"],"metadata":{"id":"4gb7JzHukubV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, so it is a table, storing the signal value for the characteristic reactant peaks (for Ge and '1' - $C_{14}H_{20}O_2$)\n","\n","As always, let's look at it:"],"metadata":{"id":"kDKxI0GZkyp5"}},{"cell_type":"code","source":["# Plotting the data\n","fig, ax1 = plt.subplots(figsize=(10,5))\n","\n","ax1.set_xlabel('LAG Liquid')\n","ax1.tick_params(axis='x',rotation=45)\n","\n","colour = '#e41a1c'\n","ax1.set_ylabel('1 Peak (signal at 13 $2 \\\\theta$)', color=colour)\n","\n","# Add 'I Peak' scatter plot\n","for liquid in peak_values['LAG Liquid'].unique():\n","    i_peak_values = peak_values[peak_values['LAG Liquid'] == liquid]['1 Peak']\n","    ax1.scatter([liquid]*len(i_peak_values), i_peak_values, color=colour)\n","\n","ax1.tick_params(axis='y',labelcolor=colour)\n","ax1.set_ylim([0,17])\n","\n","ax2 = ax1.twinx()\n","\n","colour = '#377eb8'\n","ax2.set_ylabel('Ge Peak (signal at 27 $2 \\\\theta$)', color=colour)\n","\n","# Add 'Ge Peak' scatter plot\n","for liquid in peak_values['LAG Liquid'].unique():\n","    ge_peak_values = peak_values[peak_values['LAG Liquid'] == liquid]['Ge Peak']\n","    ax2.scatter([liquid]*len(ge_peak_values), ge_peak_values, color=colour)\n","\n","ax2.tick_params(axis='y', labelcolor=colour)\n","ax2.set_ylim([0,17])\n","\n","plt.show()"],"metadata":{"id":"b3o3aiVY4Pt_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Talk with your neighbours - what is the above showing us?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"7yTuNMndlIoU"}},{"cell_type":"markdown","source":["**some notes for you**"],"metadata":{"id":"-mnEfNallNxb"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ELYJzCGglOX7"}},{"cell_type":"markdown","source":["I don't know about you, but looking at repeated measurements makes me think we need some stats!"],"metadata":{"id":"KWp6NNMzlh-R"}},{"cell_type":"markdown","source":["## Within-sample variation\n","For each sample, a variance can be calculated by using the **sample variance** (like sample standard deviation) we talked about in [Class 4](https://colab.research.google.com/drive/1KJVhs9Wmx7ff7NyPytj8p9v7u9Zmbhgx?usp=sharing).\n","\n","$$s^2 = \\frac{\\sum \\left( x_i -\\bar{x}\\right)^2}{n-1} $$\n","\n","This tells us the variance associated with each LAG liquid. Some intrumental techniques are extremely repeatable, and some are not. If you don't know how repeatable your methods are, it's always a good idea to see if you can create replicates. Without replicates, it can be hard to know if values are really as different from each other as they look.\n","\n","We can write the above as a simple function."],"metadata":{"id":"aRSd8a2cf1Rj"}},{"cell_type":"code","source":["def within_sample_variance(x):\n","    x_bar = np.mean(x)  # mean of the sample\n","    diff_squared = (x - x_bar) ** 2  # squared difference from mean for each observation\n","    s_squared = np.sum(diff_squared) / (len(x) - 1)  # sample variance\n","    return s_squared"],"metadata":{"id":"BRg8-nW7pOut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And test it for all our data."],"metadata":{"id":"RYS-FS0IpjaA"}},{"cell_type":"code","source":["# Calculate mean and variance for each liquid for each Peak\n","liquid_stats = peak_values.groupby('LAG Liquid').agg(\n","    {'1 Peak': ['mean', within_sample_variance],\n","     'Ge Peak': ['mean', within_sample_variance]}).reset_index()\n","liquid_stats"],"metadata":{"id":"4Aoc78xoB1Eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This did something fun! Check out the [`pd.DataFrame.agg`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html) function to see how it that worked out.\n","\n","Let's add those basic stats to our plot too."],"metadata":{"id":"RTsTncvqB0ne"}},{"cell_type":"markdown","source":["# Adding some stats to our plot"],"metadata":{"id":"wuyWCDKG2pvw"}},{"cell_type":"code","source":["# Improved version\n","fig, ax1 = plt.subplots(figsize=(10,5))\n","\n","ax1.set_xlabel('LAG Liquid')\n","ax1.tick_params(axis='x', rotation=45)\n","\n","colour = '#e41a1c'\n","ax1.set_ylabel('1 Peak (signal at 13 $2 \\\\theta$)', color=colour)\n","\n","# For each unique liquid\n","for liquid in peak_values['LAG Liquid'].unique():\n","    # Plot scatter points\n","    i_peak_values = peak_values[peak_values['LAG Liquid'] == liquid]['1 Peak']\n","    ax1.scatter([liquid]*len(i_peak_values), i_peak_values, color=colour, alpha=0.4)\n","\n","    # Plot mean and variance\n","    mean = liquid_stats.loc[liquid_stats['LAG Liquid'] == liquid, ('1 Peak', 'mean')]\n","    variance = liquid_stats.loc[liquid_stats['LAG Liquid'] == liquid, ('1 Peak', 'within_sample_variance')]\n","    ax1.errorbar(liquid, mean, yerr=np.sqrt(variance), color=colour, fmt='o')\n","\n","ax1.tick_params(axis='y', labelcolor=colour)\n","ax1.set_ylim([0,17])\n","\n","ax2 = ax1.twinx()\n","\n","colour = '#377eb8'\n","ax2.set_ylabel('Ge Peak (signal at 27 $2 \\\\theta$)', color=colour)\n","\n","for liquid in peak_values['LAG Liquid'].unique():\n","    ge_peak_values = peak_values[peak_values['LAG Liquid'] == liquid]['Ge Peak']\n","    ax2.scatter([liquid]*len(ge_peak_values), ge_peak_values, color=colour, alpha=0.4)\n","\n","    mean = liquid_stats.loc[liquid_stats['LAG Liquid'] == liquid, ('Ge Peak', 'mean')]\n","    variance = liquid_stats.loc[liquid_stats['LAG Liquid'] == liquid, ('Ge Peak', 'within_sample_variance')]\n","    ax2.errorbar(liquid, mean, yerr=np.sqrt(variance), color=colour, fmt='o')\n","\n","ax2.tick_params(axis='y', labelcolor=colour)\n","ax2.set_ylim([0,17])\n","\n","plt.show()"],"metadata":{"id":"QMhhzoUppn4m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, it might be obvious to you that one of the liquids is significantly different than the others. We can be precise about it though."],"metadata":{"id":"LuWBkhwFnEuS"}},{"cell_type":"markdown","source":["# ANOVA"],"metadata":{"id":"nrUcGGdLng_3"}},{"cell_type":"markdown","source":["If you have the means and variances for each condition (LAG liquid), as we do, we can run hypothesis tests to determine if there are statistically significant differences between the groups. ANOVA (Analysis of Variance) is commonly used when dealing with more than two groups/conditions.\n","\n","If using ANOVA, the null hypothesis is that all samples have the same mean, while the alternate hypothesis is that at least one group (LAG liquid) has a different mean. If the p-value is less than your chosen significance level (typically 0.05), you can reject the null hypothesis and conclude that the differences are statistically significant. P-values are the probability of observing a set of data (ie. observing what looks to be different means), given that the null hypothesis is true (ie. that they actually have the same mean). We'll go more into them later.\n","\n","Before running ANOVA, you need to ensure the assumptions of normality, equal variances (homoscedasticity), and independent observations are met. Post-hoc tests such as Tukey's test can be used after ANOVA to determine which group(s) is different.\n","\n","In this case, the variance between our samples should be from the method (perhaps our milling consistency and, importantly, the PXRD precision). Since this is fake data, I made the replicates with equal noise! That let's us assume the variance is the same for each of these.\n","\n","How to run an ANOVA?\n","\n"],"metadata":{"id":"VDyh_Pc4niF4"}},{"cell_type":"markdown","source":["## Step 1: Compute the Within Group Variance.\n","\n","You've already done this with our `within_sample_variance()` function, which calculates the average squared difference of each observation from the mean of its sample."],"metadata":{"id":"LmGUO3ESo2B3"}},{"cell_type":"code","source":["variance_1 = liquid_stats[('1 Peak', 'within_sample_variance')].values\n","variance_Ge = liquid_stats[('Ge Peak', 'within_sample_variance')].values"],"metadata":{"id":"hKc8ZJDcqQ2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Compute the Between Group Variance.\n","\n","This involves comparing the mean of each sample to the mean of all observations.\n","\n","$$s_{b}^{2} = \\frac{\\sum (n_j (\\bar{x_{j}} - \\bar{x})^{2})}{k - 1}$$\n","\n","where:\n","\n","- $s_{b}^{2}$ is the between-group variance\n","- $n_j$ is the size (number of observations) of the $j$th sample\n","- $\\bar{x_{j}}$ is the mean of the $j$th sample\n","- $\\bar{x}$ is the mean of all observations\n","- $k$ is the number of samples\n","\n","Here's a function for that:\n"],"metadata":{"id":"uiHIwy3eo9_2"}},{"cell_type":"code","source":["def between_sample_variance(sample_means, overall_mean, sample_sizes):\n","    diff_squared = (sample_means - overall_mean) ** 2  # squared difference between each sample mean and the overall mean\n","    s_squared = np.sum(diff_squared * sample_sizes) / (len(sample_means) - 1)  # sample variance; note that this is weighted by sample size\n","    return s_squared"],"metadata":{"id":"p8vjN-hBpIJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To test it, we'll need a few more pieces of information about each sample:"],"metadata":{"id":"rbYqUWBaqain"}},{"cell_type":"code","source":["group_sizes = peak_values.groupby('LAG Liquid').size().values\n","group_means_1 = liquid_stats[('1 Peak', 'mean')].values\n","group_means_Ge = liquid_stats[('Ge Peak', 'mean')].values"],"metadata":{"id":"Rm7Oog5KqdYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, calculate the overall means:"],"metadata":{"id":"gvryPBP_qi9N"}},{"cell_type":"code","source":["overall_mean_1 = np.mean(group_means_1)\n","overall_mean_Ge = np.mean(group_means_Ge)"],"metadata":{"id":"Kntt775vqotF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we use these results to calculate the between-group variances:"],"metadata":{"id":"XDZjAFk_qlGn"}},{"cell_type":"code","source":["between_variance_1 = between_sample_variance(group_means_1, overall_mean_1, group_sizes)\n","between_variance_Ge = between_sample_variance(group_means_Ge, overall_mean_Ge, group_sizes)"],"metadata":{"id":"78jAgEyCqr6L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, compute the weighted within-sample variances (add all variances within each group divided by sum of sizes):"],"metadata":{"id":"QbRSoYuWq3Gr"}},{"cell_type":"code","source":["weighted_var_1 = np.sum(variance_1 * group_sizes) / np.sum(group_sizes)\n","weighted_var_Ge = np.sum(variance_Ge * group_sizes) / np.sum(group_sizes)"],"metadata":{"id":"qwXNCasKq3o1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Compute the F-statistic.\n","\n","\"F-statistic\" is ANOVA jargon. It is the ratio of the between-sample variance to the within-sample variance."],"metadata":{"id":"LlCx-MD0po65"}},{"cell_type":"code","source":["def f_statistic(between_variance, within_variance):\n","    return between_variance / within_variance"],"metadata":{"id":"rmmxSPycpsHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The larger the F-statistic is, the more likely it is that the differences in sample means are not due to random chance.\n"],"metadata":{"id":"JzQk0yP9pvHq"}},{"cell_type":"code","source":["f_stat_1 = f_statistic(between_variance_1, weighted_var_1)\n","f_stat_Ge = f_statistic(between_variance_Ge, weighted_var_Ge)\n","print(\"For Peak 1, F = \", str(f_stat_1), \"For Peak Ge, F = \", str(f_stat_Ge))"],"metadata":{"id":"jOUugPHVq7cX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These seems like big numbers! And larger F values suggest that the group means are significantly different (so our liquids have different performance in our synthesis).  But we need to know more."],"metadata":{"id":"JXyw3GSLrFEy"}},{"cell_type":"markdown","source":["## Step 4: Obtain the F-critical value and P-value.\n","\n","The F-critical value is looked up from the F-distribution table given the degrees of freedom and the significance level. These tables are all based on properties obtained from the normal distribution.\n","\n","Degrees of freedom for the numerator (Between Group Variance) is k-1 where k is the number of groups. For the denominator (Within Group Variance), it is N-k, where N is the total number of observations.\n","\n","If your calculated F-statistic is greater than the F-critical value, you reject the null hypothesis and conclude that there is a statistically significant difference. As for the p-value, it involves integration of the F-distribution. We won't go through that math today (because integration is another class), but we can run the stat through `scipy.stats`."],"metadata":{"id":"a5f4_5kpp0F3"}},{"cell_type":"code","source":["p_value_1 = f.sf(f_stat_1, len(group_means_1)-1, len(peak_values)-len(group_means_1))\n","p_value_Ge = f.sf(f_stat_Ge, len(group_means_Ge)-1, len(peak_values)-len(group_means_Ge))\n","print(\"For Peak 1, p = \", str(p_value_1), \"For Peak Ge, p = \", str(p_value_Ge))"],"metadata":{"id":"EF3QciD3rez7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These are very small numbers! This tells us a statistically significant thing has occured! Specifically, that not all LAG liquids are the same as far as this synthesis is concerned.\n","\n","But wait, which is the best?? To identify which sample mean is the most different, you usually perform a post-hoc test after ANOVA has found a significant result. A commonplace post-hoc test is Tukey's Honestly Significant Difference (HSD) test. I appreciate its name."],"metadata":{"id":"VtTRNmL1rw5z"}},{"cell_type":"markdown","source":["# Tukey's Honestly Significant Difference test\n","\n","The basic idea of Tukey honestly significant difference (HSD) is to say which pairs of means are significantly different from each other.\n","\n","Tukey's HSD calculates a range around each mean, and if the ranges of two means do not overlap, the test concludes that they are significantly different. The formula for Tukey's test statistic is:\n","\n","$$ q = \\frac{Y_i - Y_j}{\\sqrt{MSE/n}} $$\n","\n","where:\n","- $Y_i$ and $Y_j$ are the sample means of group i and j\n","- MSE is the mean squared error - the average of the squared difference between each observation and its group mean\n","- n is the sample size\n","\n","The test statistic q has what is known as a 'studentized range distribution' (think, 'normal' distribution, again). It is used to determine the critical value, which is compared to the calculated difference of each pair of means to determine significance. Instead of writing out all of this math ourselves, we'll use a [function](https://www.statsmodels.org/devel/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html) that does it for us."],"metadata":{"id":"K7JTZzn3sZYj"}},{"cell_type":"code","source":["# Execute Tukey's HSD\n","tukey = pairwise_tukeyhsd(peak_values['1 Peak'], peak_values['LAG Liquid'], alpha=0.05)\n","\n","# Convert the summary to dataframe for easy comparison\n","tukey_1_peak_df = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n","tukey_1_peak_df"],"metadata":{"id":"r9TnXqJitLO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","What is 'meandiff' showing? What does 'reject' mean? Jump back to the plot (**Adding some stats to our plot**) and confirm these match your intiution from looking at the data.\n","\n","\n","---\n","\n"],"metadata":{"id":"BOLsXUl5wnQQ"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"EpRkR4Ksx2sV"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"L3GHQntKx3cF"}},{"cell_type":"markdown","source":["Could we say MeOH was a better LAG liquid than PhMe based on only the '1 peak', for example? We need to be able to say if two groups (LAG liquids) are significantly different for both the 1 Peak and the Ge Peak.  **Qualitatively, we are really just saying the averages are far enough apart that the variances are not overlapping.** You can actually visually determine 1:1PhMe:H$_2$O is the right choice from the above plot. Sometimes, data points are closer together though, and that is not a given.\n","\n","Try it out looking at the Ge Peak too (how much of our other reactant makes it into the product with the different liquids - ideally, neither reactant is significantly in our final product, remember)."],"metadata":{"id":"eeQ7-TssyCl9"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Confirm to yourself that 1:1 PhMe:H₂O really is the best LAG liquid (by checking the Ge peak too).\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"l0cZQhHm1_FN"}},{"cell_type":"code","source":["# you'll need to figure out which of the above pieces of code to include!"],"metadata":{"id":"N6UlzvSy3fAS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"MOCF3cCM3h8R"}},{"cell_type":"markdown","source":["# Optimization of LAG additive volume.\n","\n","Every part of the synthesis should be optimized!\n","\n","Next, they looked at the volume of liquid added.\n"],"metadata":{"id":"HfarWtau3piG"}},{"cell_type":"code","source":["# Define the DataFrame columns\n","columns = ['LAG Liquid', 'LAG Volume (μL)', 'Pyridine (equiv)', 'Milling Time (mins)', 'Milling frequency (Hz)', 'Scale (mg)', 'Observed SM Phase', 'Observed Product Phase']\n","\n","# Initialize an empty DataFrame\n","LAG_volume_df = pd.DataFrame(columns=columns)\n","\n","# Fill in the DataFrame with your data\n","LAG_volume_df.loc[0] = ['1:1 PhMe:H₂O', 30, 2, 90, 25, 200, 'Ge. 1', '3'] # values for Entry (1)\n","LAG_volume_df.loc[1] = ['1:1 PhMe:H₂O', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (2)\n","LAG_volume_df.loc[2] = ['1:1 PhMe:H₂O', 120, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (3)\n","\n","# now let's call the dataframe to look at the table\n","LAG_volume_df"],"metadata":{"id":"4HWwTkXj0mCj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's make some fake data again based on what they provide in the supplement."],"metadata":{"id":"U6vZskIp33ga"}},{"cell_type":"code","source":["# Define parameters for the Gaussian peaks for each PXRD pattern: height, mean, width for each peak\n","all_params = [\n","    { 'params': [(1, 21, .1), (3, 25.5, .1), (15, 27, .1)], 'name': 'Ge', 'colour': '#377eb8', 'noise':.2},\n","    { 'params': [(1, 7, .1), (1, 12.2, .2), (15, 13, .1), (1, 17, .2), (1.5, 19, .2), (1, 22, .2), (1, 25, .2), (1, 30, .2)], 'name': '1', 'colour': '#e41a1c', 'noise':.3},\n","    { 'params': [(15, 8.5, .1), (6, 10, .4), (6, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': '30 µL', 'colour': '#4daf4a', 'noise':.5},\n","    {  'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': '60 µL', 'colour': '#4daf4a', 'noise':.5},\n","    {  'params': [(3, 8.5, .1), (5, 10, .4), (15, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': '120 µL', 'colour': '#4daf4a', 'noise':.5},\n","]"],"metadata":{"id":"QE6ZbvrrNLwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And they found:\n","> The resulting green/brown crude solids were analyzed by PXRD fig S3 ). The optimal liquid additive\n","volume was found to be 60 µL\n","\n","How? Let's look at our version of their plot for this step:"],"metadata":{"id":"hjNZlk98NMmj"}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Generate and plot PXRD patterns for each parameter set\n","for i, param_dict in enumerate(all_params):\n","\n","    # Generate noise and add it to the y-data\n","    np.random.seed(0)  # For reproducibility\n","    noise = np.random.normal(0, param_dict['noise'], theta.shape)\n","\n","    # Generate the y-data for the PXRD pattern\n","    y = sum(gaussian(theta, height, mean, width) for height, mean, width in param_dict['params'])\n","    y += noise\n","\n","    # Add the offset for easier visualization\n","    y -= i * 20\n","\n","    # Plot the PXRD pattern\n","    ax.plot(theta, y, lw=3, color=param_dict['colour'])\n","\n","    # Create a text label for the PXRD pattern\n","    ax.text(42, -i*20, param_dict['name'], verticalalignment='center', color=param_dict['colour'])\n","\n","# Set the x-ticks to start at 4 and increment by 5\n","ax.set_xticks(range(4, 40, 5))\n","\n","# Remove y-axis ticks\n","ax.yaxis.set_ticklabels([])\n","\n","# Hide grid lines\n","ax.grid(False)\n","\n","# Set the x-axis label\n","ax.set_xlabel('$2 \\\\theta$')\n","\n","plt.show()"],"metadata":{"id":"LTtCpC6-Ka3n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","How come 60 µL was the winner?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"IDVjLH1SNHAE"}},{"cell_type":"markdown","source":["**This can be qualitative - you'll do a quantitative example on the homework**"],"metadata":{"id":"mTpRi4RZ4X1h"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"xUdCbcud4cMT"}},{"cell_type":"markdown","source":["# Optimization of Py equivalents.\n","\n","Let's look at their next step.\n","\n","> The resulting green/brown crude solids were analyzed by PXRD fig S4). The optimal Py stoichiometry was found to be 2 equivalents"],"metadata":{"id":"VGX3f01W4ruz"}},{"cell_type":"code","source":["# Define the DataFrame columns\n","columns = ['LAG Liquid', 'LAG Volume (μL)', 'Pyridine (equiv)', 'Milling Time (mins)', 'Milling frequency (Hz)', 'Scale (mg)', 'Observed SM Phase', 'Observed Product Phase']\n","\n","# Initialize an empty DataFrame\n","LAG_volume_df = pd.DataFrame(columns=columns)\n","\n","# Fill in the DataFrame with your data\n","LAG_volume_df.loc[0] = ['1:1 PhMe:H₂O', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (1)\n","LAG_volume_df.loc[1] = ['1:1 PhMe:H₂O', 60, 3, 90, 25, 200, 'Ge. 1*', '3'] # values for Entry (2)\n","LAG_volume_df.loc[2] = ['1:1 PhMe:H₂O', 60, 4, 90, 25, 200, 'Ge. 1*', '3'] # values for Entry (3)\n","\n","# now let's call the dataframe to look at the table\n","LAG_volume_df"],"metadata":{"id":"EA-4S68RNZqW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And a visualization:"],"metadata":{"id":"twSiR-8y0lBL"}},{"cell_type":"code","source":["# Define parameters for the Gaussian peaks for each PXRD pattern: height, mean, width for each peak\n","all_params = [\n","    { 'params': [(1, 21, .1), (3, 25.5, .1), (15, 27, .1)], 'name': 'Ge', 'colour': '#377eb8', 'noise':.2},\n","    { 'params': [(1, 7, .1), (1, 12.2, .2), (15, 13, .1), (1, 17, .2), (1.5, 19, .2), (1, 22, .2), (1, 25, .2), (1, 30, .2)], 'name': '1', 'colour': '#e41a1c', 'noise':.3},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': 'Py (2 equiv)', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': 'Py (3 equiv)', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': 'Py (4 equiv)', 'colour': '#4daf4a', 'noise':.5},\n","]"],"metadata":{"id":"NR_BYAfIN8Nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've set up the fake peak parameters."],"metadata":{"id":"jJvp_IASN5p6"}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Generate and plot PXRD patterns for each parameter set\n","for i, param_dict in enumerate(all_params):\n","\n","    # Generate noise and add it to the y-data\n","    np.random.seed(0)  # For reproducibility\n","    noise = np.random.normal(0, param_dict['noise'], theta.shape)\n","\n","    # Generate the y-data for the PXRD pattern\n","    y = sum(gaussian(theta, height, mean, width) for height, mean, width in param_dict['params'])\n","    y += noise\n","\n","    # Add the offset for easier visualization\n","    y -= i * 20\n","\n","    # Plot the PXRD pattern\n","    ax.plot(theta, y, lw=3, color=param_dict['colour'])\n","\n","    # Create a text label for the PXRD pattern\n","    ax.text(42, -i*20, param_dict['name'], verticalalignment='center', color=param_dict['colour'])\n","\n","# Set the x-ticks to start at 4 and increment by 5\n","ax.set_xticks(range(4, 40, 5))\n","\n","# Remove y-axis ticks\n","ax.yaxis.set_ticklabels([])\n","\n","# Hide grid lines\n","ax.grid(False)\n","\n","# Set the x-axis label\n","ax.set_xlabel('$2 \\\\theta$')\n","\n","plt.show()"],"metadata":{"id":"LvwGmpWvOLb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Hmm, how did they optimize this one? What was different about this step?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"24k_7kirOQ0T"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"9tBKG5qS5UDr"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Vg0pBYnO5Vhn"}},{"cell_type":"markdown","source":["# Optimization of milling time.\n","\n","> The resulting green/brown crude solids were analyzed by PXRD (fig S5). The optimal milling time changes. The optimal milling time changes to 180 minutes for near quantitative conversion of starting materials as observed by PXRD\n","\n","Alright, let's see how this goes:"],"metadata":{"id":"pDQDVEjE5X31"}},{"cell_type":"code","source":["# Define the DataFrame columns\n","columns = ['LAG Liquid', 'LAG Volume (μL)', 'Pyridine (equiv)', 'Milling Time (mins)', 'Milling frequency (Hz)', 'Scale (mg)', 'Observed SM Phase', 'Observed Product Phase']\n","\n","# Initialize an empty DataFrame\n","LAG_volume_df = pd.DataFrame(columns=columns)\n","\n","# Fill in the DataFrame with your data\n","LAG_volume_df.loc[0] = ['1:1 PhMe:H₂O', 60, 2, 30, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (1)\n","LAG_volume_df.loc[1] = ['1:1 PhMe:H₂O', 60, 2, 60, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (2)\n","LAG_volume_df.loc[2] = ['1:1 PhMe:H₂O', 60, 2, 90, 25, 200, 'Ge. 1', '3, 3a'] # values for Entry (3)\n","LAG_volume_df.loc[3] = ['1:1 PhMe:H₂O', 60, 2, 180, 25, 200, 'trace Ge. 1', '3a'] # values for Entry (4)\n","\n","# now let's call the dataframe to look at the table\n","LAG_volume_df"],"metadata":{"id":"HYUGe1O6OQ96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And some fake data"],"metadata":{"id":"x2AwzrEFPUV7"}},{"cell_type":"code","source":["# Define parameters for the Gaussian peaks for each PXRD pattern: height, mean, width for each peak\n","all_params = [\n","    { 'params': [(1, 21, .1), (3, 25.5, .1), (15, 27, .1)], 'name': 'Ge', 'colour': '#377eb8', 'noise':.2},\n","    { 'params': [(1, 7, .1), (1, 12.2, .2), (15, 13, .1), (1, 17, .2), (1.5, 19, .2), (1, 22, .2), (1, 25, .2), (1, 30, .2)], 'name': '1', 'colour': '#e41a1c', 'noise':.3},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (7, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (6, 27, .1), (2, 30, .2)], 'name': '30 mins', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (4, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (4, 27, .1), (2, 30, .2)], 'name': '60 mins', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (2, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (2, 27, .1), (2, 30, .2)], 'name': '90 mins', 'colour': '#4daf4a', 'noise':.5},\n","    { 'params': [(15, 8.5, .1), (10, 10, .4), (1, 13, .1), (2, 17, .2), (2, 19, .2), (2, 22, .2), (2, 25, .2), (2, 25.5, .1), (0.5, 27, .1), (1, 30, .2)], 'name': '180 mins', 'colour': '#4daf4a', 'noise':.5},\n","]"],"metadata":{"id":"L8eNJYiDPhGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the plot:"],"metadata":{"id":"Ib_m4vH9Phis"}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Generate and plot PXRD patterns for each parameter set\n","for i, param_dict in enumerate(all_params):\n","\n","    # Generate noise and add it to the y-data\n","    np.random.seed(0)  # For reproducibility\n","    noise = np.random.normal(0, param_dict['noise'], theta.shape)\n","\n","    # Generate the y-data for the PXRD pattern\n","    y = sum(gaussian(theta, height, mean, width) for height, mean, width in param_dict['params'])\n","    y += noise\n","\n","    # Add the offset for easier visualization\n","    y -= i * 20\n","\n","    # Plot the PXRD pattern\n","    ax.plot(theta, y, lw=3, color=param_dict['colour'])\n","\n","    # Create a text label for the PXRD pattern\n","    ax.text(42, -i*20, param_dict['name'], verticalalignment='center', color=param_dict['colour'])\n","\n","# Set the x-ticks to start at 4 and increment by 5\n","ax.set_xticks(range(4, 40, 5))\n","\n","# Remove y-axis ticks\n","ax.yaxis.set_ticklabels([])\n","\n","# Hide grid lines\n","ax.grid(False)\n","\n","# Set the x-axis label\n","ax.set_xlabel('$2 \\\\theta$')\n","\n","plt.show()"],"metadata":{"id":"VhxuD4yrPUfH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, something important should be noticed here...\n","\n"],"metadata":{"id":"xnRT47e2PaKu"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","180 minutes is the optimal milling time. What value of milling time was used for all of the above? Does this... matter?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"acPRrE4L5wk-"}},{"cell_type":"markdown","source":["**Notes**"],"metadata":{"id":"FJ26aWp854Mb"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Bkd5WIhV55sY"}},{"cell_type":"markdown","source":["Optimization is non-trivial. What if some other grouping of conditions - even some other LAG liquid - with a different milling time and Pyridine equivalent - would have actually been better?\n","\n","This style of optimization - change one variable at a time and find the best then move on to the next - is quite common. But it can have errors when systems have non-linearities (ie. the most efficient set of conditions for one solvent may be very different for another).\n","\n","So how would we account for this? Do we need to search every possible combination of things? (That is a lot of things!) I bet you want to know the answer to this..."],"metadata":{"id":"rBrjcb9G56Td"}},{"cell_type":"markdown","source":["# Submit your notebook\n","\n","But... It's time to download your notebook and submit it on Canvas. Go to the File menu and click **Download** -> **Download .ipynb**\n","\n","Then, go to **Canvas** and **submit your assignment** on the assignment page. Once it is submitted, swing over to the homework now and start working through the paper."],"metadata":{"id":"FbAnpHqm72Vd"}},{"cell_type":"markdown","source":["# Another way\n","You can stop here and submit your notebook if you are not excited for more... BUT, if you want to see another way to optimize a reaction, read on. If you haven't done linear algebra yet, maybe also stop...\n","\n","Two Mudd profs (Prof. Van Ryswyk and Prof. Van Hecke) wrote a nice, simple [paper](https://drive.google.com/file/d/1kJsliksMzu2i9wUqIg6w5w6zHI8dkOk9/view?usp=share_link) on experimental optimization for students. They showed a different - linear algebra based - way of optimizing a reaction that doesn't involve testing every single combination of things! Much of the below math comes from Prof. VR (and I translated it into Python)."],"metadata":{"id":"rc9cSDGGOwNf"}},{"cell_type":"markdown","source":["We will walk (run?) through the optimization of the synthesis of acetylferrocene using VR/VH scheme to optimize time, temperature, and mole ratio (in this example, they had three parameters to vary). Reading the paper along with it will make a lot more sense (it was too much for me to type out though!).\n","\\begin{align}\n","\\text{Fe}\\left(\\text{C}_5\\text{H}_5\\right)_2+\\left(\\text{CH}_3\\text{CO}\\right)_2 \\rightarrow \\left(\\text{C}_5\\text{H}_5\\right)\\text{Fe}\\left(\\text{C}_5\\text{H}_4\\right)\\text{COCH}_3+\\text{Fe}\\left[\\left(\\text{C}_5\\text{H}_5\\right)\\text{COCH}_3\\right]_2 + ...\n","\\end{align}\n","\n","---\n","![https://kavassalis.space/s/ferrocene_scence.png](https://kavassalis.space/s/ferrocene_scence.png)\n","\n","\n","---\n"],"metadata":{"id":"3KDG6Zwpviaj"}},{"cell_type":"markdown","source":["From Prof. VR:\n","> The experimental design aims to establish a response surface in the minimum number of experiments. Variables are encoded, and a central composite design is used to efficiently explore the response surface.\n","\n","I wrote out the functions they used."],"metadata":{"id":"QRyPcNWCxz4i"}},{"cell_type":"code","source":["# encoding equations for dimensionless time, temperature, and mole ratio\n","def encode_time(t):\n","  '''t in seconds'''\n","  return (t - 120) / 90\n","\n","def encode_temp(T):\n","  '''T in degrees Celsius'''\n","  return (T - 100) / 15\n","\n","def encode_ratio(R):\n","  '''R as moles acetic anhydride : moles ferrocene'''\n","  return (R - 10) / 7"],"metadata":{"id":"73c8NOwrvih9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's make sure those functions work."],"metadata":{"id":"Eh3yU8GDyQ5E"}},{"cell_type":"code","source":["# Example encoding\n","t_encoded = encode_time(130)  # Example time in seconds\n","T_encoded = encode_temp(105)  # Example temperature in °C\n","R_encoded = encode_ratio(12)  # Example mole ratio"],"metadata":{"id":"CVU8ZyH5yRA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, now, it starts to get a bit jargon heavy (if you are here, make sure you have the paper open too!)."],"metadata":{"id":"kHhBYlPPvlFf"}},{"cell_type":"markdown","source":["## Central composite design matrix for three factors $(k=3)$.\n","> This is a full $2^3$ design with star augmentation along the principal axes. The second-order fitting equation with cross-terms is:\n","\\begin{align}\n","y=b_0+b_1x_1 + b_2x_2 + b_3x_3 + b_4x_1^2 + b_5x_2^2 + b_6x_3^2 + b_7x_1x_2 + b_8x_1x_3 + b_9x_2x_3\n","\\end{align}"],"metadata":{"id":"dSd9PtbJArQB"}},{"cell_type":"code","source":["# star augmentation\n","alpha = 1.2\n","\n","# The design matrix\n","# row format: 1, x1, x2, x3, x1^2, x2^2, x3^2, x1x2, x1x3, x2x3\n","X = np.array([\n","    [1, -1, -1, -1, 1, 1, 1, 1, 1, 1],\n","    [1, -1, -1, 1, 1, 1, 1, 1, -1, -1],\n","    [1, -1, 1, -1, 1, 1, 1, -1, 1, -1],\n","    [1, -1, 1, 1, 1, 1, 1, -1, -1, 1],\n","    [1, 1, -1, -1, 1, 1, 1, -1, -1, 1],\n","    [1, 1, -1, 1, 1, 1, 1, -1, 1, -1],\n","    [1, 1, 1, -1, 1, 1, 1, 1, -1, -1],\n","    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","    [1, -alpha, 0, 0, alpha**2, 0, 0, 0, 0, 0],\n","    [1, alpha, 0, 0, alpha**2, 0, 0, 0, 0, 0],\n","    [1, 0, -alpha, 0, 0, alpha**2, 0, 0, 0, 0],\n","    [1, 0, alpha, 0, 0, alpha**2, 0, 0, 0, 0],\n","    [1, 0, 0, -alpha, 0, 0, alpha**2, 0, 0, 0],\n","    [1, 0, 0, alpha, 0, 0, alpha**2, 0, 0, 0],\n","])\n","\n","\n","# yield result from the experiments\n","y = np.array([0, 0.032, 0.008, 0.048, 0.427, 0.215, 0.516, 0.281, 0.648, 0.168, 0.613, 0.453, 0.488, 0.499, 0.396])\n","\n","# least squares estimate of model coefficients\n","beta = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n","print('β:', beta)"],"metadata":{"id":"l1po2uxuvlMi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That sure seems like a lot to take in! Let's make a plot that maybe looks more straightforward (we like residuals, right?)"],"metadata":{"id":"0Mb7zZYEvnSZ"}},{"cell_type":"code","source":["# residuals\n","residuals = y - X.dot(beta)\n","\n","# Plot residuals\n","plt.plot(residuals, linestyle='', marker='o')\n","plt.xlabel('run #')\n","plt.ylabel('y observed - y predicted')\n","plt.title('Residuals')\n","plt.show()"],"metadata":{"id":"V7gy5ASWvnZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How does this let us optimize things? This looks like scattered runs with varying residual values?\n","\n","Back to the paper:"],"metadata":{"id":"gPidQoAt8Xa2"}},{"cell_type":"markdown","source":["> Decompose β in preparation for eigenvalue and stationary point calculation. Vector b contains the first-order coefficients b1, b2, and b3. Matrix B has the coefficients for the squared terms, b4 through b6, on the diagonal and symmetrical cross terms/2 derived from b7 through b9.\n","\n","We can write that out:"],"metadata":{"id":"PxcpZbFRvrKT"}},{"cell_type":"code","source":["# decomposition of β\n","b = np.array([0.173, 0.0203, -0.0458])\n","B = np.array([[-0.187, 0.00819, -0.0324],\n","              [0.00819, -0.131, -0.000938],\n","              [-0.0324, -0.000938, -0.147]])\n"],"metadata":{"id":"71PJ8NBrvqsn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> The eigenvectors of B describe a linear combination of our time, temperature, and mole ratio axes such that the origin is now at the stationary point\n","\n","(again, this will be a lot of jargon and a very fast pace if this math isn't comfortable - that is okay - this is just for flavour).\n","\n","> The eigenvalues of B tell us if the stationary point is a maxima, minima, or saddle point with respect to yield. If we have found a maxima, then the eigenvalues will all be negative. If the signs of the eigenvalues differ, then we have found a saddle point and we should proceed in the direction of the eigenvector with the largest positive eigenvalue in order to maximize our yield."],"metadata":{"id":"3qVKd6EfvsoR"}},{"cell_type":"code","source":["# canonical analysis\n","w, v = np.linalg.eig(B)\n","print('w:', w)\n","print('λ:', v)"],"metadata":{"id":"GNr2y8VsX-MA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's calculate the stationary point"],"metadata":{"id":"zHK8xvI0YBbw"}},{"cell_type":"code","source":["# stationary point\n","xs = -np.linalg.inv(B).dot(b) / 2\n","print('xs:', xs)"],"metadata":{"id":"6bnRPoGMvsxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The yield of acetylferrocene (y) can be modeled as follows:\n","\n","$$\n","y = f(t, T, R) + \\epsilon\n","$$\n","\n","where $\\epsilon$ represents the residual error in each run."],"metadata":{"id":"udm-JLzYAlDr"}},{"cell_type":"markdown","source":["Next, calculate yield at stationary point and express as a percent"],"metadata":{"id":"1Nd448XkxZHU"}},{"cell_type":"code","source":["ys = beta[0] + 0.5*np.dot(xs, b)\n","\n","ys_percentage = 100 * ys  # Convert ys to a percentage\n","formatted_yield = f\"{ys_percentage:.2f}% yield\"  # Format to 2 decimal places and add unit\n","\n","print(formatted_yield)"],"metadata":{"id":"8dRWlVxNyW-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And for time:"],"metadata":{"id":"PCmKzANOzC_t"}},{"cell_type":"code","source":["# Assume xs[0] contains the calculated stationary point value for x1 (time)\n","time_opt = xs[0] * 90 + 120  # Applying the given formula to calculate optimal time\n","\n","# Formatting the output to 3 significant figures in Python can be done in various ways.\n","# Here's a straightforward method using formatted string literals for 3 decimal places:\n","formatted_time = f\"{time_opt:.3f} s\"\n","\n","print(formatted_time)"],"metadata":{"id":"vkfvoc8jz2kS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And temperature:"],"metadata":{"id":"vUkXiLWU9jf3"}},{"cell_type":"code","source":["temp_opt = xs[1] * 15 + 100  # Apply the formula to calculate optimal temperature\n","\n","# Format for display with 3 decimal places and unit °C\n","formatted_temp = f\"{temp_opt:.3f} °C\"\n","\n","print(formatted_temp)"],"metadata":{"id":"3LYFSxPB0c_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And mole ratio:"],"metadata":{"id":"VjPzGGMl9lhG"}},{"cell_type":"code","source":["mole_ratio_opt = xs[2] * 7 + 10  # Apply the provided formula\n","\n","# Formatting precision in Python often focuses on decimal places rather than significant figures.\n","# Here, we aim for two decimal places and manually add the unit of measure.\n","formatted_mole_ratio = f\"{mole_ratio_opt:.2f} mole ratio acetic anhydride:ferrocene\"\n","\n","print(formatted_mole_ratio)"],"metadata":{"id":"cqAgA3ll0xck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now visualize the results as a 3D heat map. First, we need to set up the experimental data."],"metadata":{"id":"rrc-cjCj04Ly"}},{"cell_type":"code","source":["# 4D data set consisting of x1, x2, x3, yield\n","AFc = [\n","    (-1, -1, -1, 0.0),\n","    (-1, -1, 1, 0.032),\n","    (-1, 1, -1, 0.008),\n","    (-1, 1, 1, 0.048),\n","    (1, -1, -1, 0.427),\n","    (1, -1, 1, 0.215),\n","    (1, 1, -1, 0.516),\n","    (1, 1, 1, 0.281),\n","    (0, 0, 0, 0.648),\n","    (-1.2, 0, 0, 0.168),\n","    (1.2, 0, 0, 0.613),\n","    (0, -1.2, 0, 0.453),\n","    (0, 1.2, 0, 0.488),\n","    (0, 0, -1.2, 0.499),\n","    (0, 0, 1.2, 0.396)\n","]\n","\n","AFc_np = np.array(AFc)"],"metadata":{"id":"fv9UmOLP1OVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can make a plot:"],"metadata":{"id":"BAD1uK271mWY"}},{"cell_type":"code","source":["# Extracting x1 (time), x2 (temp), x3 (mole ratio), and yields\n","x1 = AFc_np[:,0]\n","x2 = AFc_np[:,1]\n","x3 = AFc_np[:,2]\n","yields = AFc_np[:,3]\n","\n","# Setup for 3D scatter plot\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Scatter plot with colormap\n","img = ax.scatter(x1, x2, x3, c=yields, cmap=cmap_temp)\n","\n","# Colour bar indicating yield values\n","cbar = fig.colorbar(img, ax=ax, shrink=0.8, pad=0.1)\n","cbar.set_label('Yield')\n","\n","# Setting labels\n","ax.set_xlabel('Scaled Time')\n","ax.set_ylabel('Scaled Temp')\n","ax.set_zlabel('Scaled Mol Ratio')\n","\n","# Plot title\n","plt.title('Optimization of Acetylferrocene Synthesis')\n","\n","plt.show()"],"metadata":{"id":"mBAOOG8q1mdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, so we want yield to be a maximum, and have a 3-variable parameter space. You can see the experiments that have been done and they appear to be somewhat randomly sampling said parameter space (this is good! we'll talk more later on how to set up this random sampling)\n","\n","It's kind of hard to look at this plot and say where the optimized values occur though. Let's interpolate things to make something a bit easier to look at."],"metadata":{"id":"OSJaWdY394G0"}},{"cell_type":"code","source":["grid_x, grid_y, grid_z = np.mgrid[\n","    AFc_np[:,0].min():AFc_np[:,0].max():100j,\n","    AFc_np[:,1].min():AFc_np[:,1].max():100j,\n","    AFc_np[:,2].min():AFc_np[:,2].max():100j\n","]\n","\n","points = AFc_np[:, :3]\n","values = AFc_np[:, 3]\n","\n","grid_yields = griddata(points, values, (grid_x, grid_y, grid_z), method='linear')"],"metadata":{"id":"_4UruPAV2ZKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's plot the interpolated data:"],"metadata":{"id":"5e6ddvh--RFK"}},{"cell_type":"code","source":["# Define the grid where we want to interpolate.\n","grid_x, grid_y, grid_z = np.mgrid[-2:2:100j, -2:2:100j, -2:2:100j]\n","\n","# Perform the 3D interpolation.\n","grid = griddata(points, values, (grid_x, grid_y, grid_z), method='linear')\n","\n","# Now create the 3D scatter plot.\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","scatter = ax.scatter(grid_x, grid_y, grid_z, c=grid.ravel(), cmap=cmap_temp)\n","\n","# Colour bar indicating yield values\n","cbar = fig.colorbar(img, ax=ax, shrink=0.8, pad=0.1)\n","cbar.set_label('Yield')\n","\n","# Customizing the plot\n","ax.set_xlabel('Scaled time')\n","ax.set_ylabel('Scaled temp')\n","ax.set_zlabel('Scaled mol ratio')\n","ax.set_title('Optimization of acetylferrocene synthesis')\n","\n","plt.show()"],"metadata":{"id":"D8ipnWxn25il"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that helps us see what ranges of parameters might have optimal yield.\n","\n","It would be nice to zoom in though...\n","\n","Look!"],"metadata":{"id":"fq80Gw4j-ptX"}},{"cell_type":"code","source":["# Separate the coordinates (x1, x2, x3) from the yield values\n","points = AFc_np[:, :3]   # x1, x2, x3 coordinates\n","values = AFc_np[:, 3]    # yields\n","\n","# Define the grid where we want to interpolate\n","grid_x, grid_y, grid_z = np.mgrid[\n","    AFc_np[:,0].min():AFc_np[:,0].max():50j,\n","    AFc_np[:,1].min():AFc_np[:,1].max():50j,\n","    AFc_np[:,2].min():AFc_np[:,2].max():50j\n","]\n","\n","# Perform the 3D interpolation\n","grid_values = griddata(points, values, (grid_x, grid_y, grid_z), method='linear')\n","\n","# Compute global min and max of the yield\n","min_yield = np.nanmin(AFc_np[:, 3])\n","max_yield = np.nanmax(AFc_np[:, 3])\n","\n","fig = go.Figure()\n","\n","# Add invisible trace with global min yield\n","fig.add_trace(go.Volume(\n","    x=[AFc_np[0, 0]], y=[AFc_np[0, 1]], z=[AFc_np[0, 2]],\n","    value=[min_yield], opacity=0.))\n","\n","# Add invisible trace with global max yield\n","fig.add_trace(go.Volume(\n","    x=[AFc_np[0, 0]], y=[AFc_np[0, 1]], z=[AFc_np[0, 2]],\n","    value=[max_yield], opacity=0.))\n","\n","# Add your main volume plot\n","fig.add_trace(go.Volume(\n","    x=grid_x.flatten(),\n","    y=grid_y.flatten(),\n","    z=grid_z.flatten(),\n","    value=grid_values.flatten(),\n","    isomin=grid_values.min(),\n","    isomax=grid_values.max(),\n","    opacity=0.1,\n","    surface_count=16,\n","    colorscale=colourscale))\n","\n","fig.show()"],"metadata":{"id":"jB8QhmjR4JhH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, that doesn't actually seem as helpful as I wanted (but it can be optimized more). There are several different ways to visualize this kind of space.\n","\n","We can just look at slices of the observations:"],"metadata":{"id":"srNNPaWI_cC8"}},{"cell_type":"code","source":[" # Create masks for specific slices (you can adjust these as needed)\n","t_mask = np.abs(AFc_np[:, 0] - 0) < 1e-3  # slice at scaled time = 0\n","temp_mask = np.abs(AFc_np[:, 1] - 0) < 1e-3  # slice at scaled temp = 0\n","mol_mask = np.abs(AFc_np[:, 2] - 0) < 1e-3  # slice at scaled mol ratio = 0\n","\n","# Compute global min and max of the yield\n","# This way, the same color scale will be used for all of your contour plots.\n","min_yield = np.nanmin(AFc_np[:, 3])\n","max_yield = np.nanmax(AFc_np[:, 3])\n","\n","\n","# Draw the slice plots\n","fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n","\n","# Yield vs mol_ratio and time (temp = 0)\n","ax[0].scatter(AFc_np[t_mask][:, 2], AFc_np[t_mask][:, 0], c=AFc_np[t_mask][:, 3], cmap=cmap_temp, vmin=min_yield, vmax=max_yield)\n","ax[0].set_xlabel('Scaled Mol Ratio')\n","ax[0].set_ylabel('Scaled Time')\n","ax[0].set_title('Scaled Temp = 0')\n","ax[0].grid(True)\n","\n","# Yield vs time and temp (mol_ratio = 0)\n","ax[1].scatter(AFc_np[mol_mask][:, 0], AFc_np[mol_mask][:, 1], c=AFc_np[mol_mask][:, 3], cmap=cmap_temp, vmin=min_yield, vmax=max_yield)\n","ax[1].set_xlabel('Scaled Time')\n","ax[1].set_ylabel('Scaled Temp')\n","ax[1].set_title('Scaled Mol Ratio = 0')\n","ax[1].grid(True)\n","\n","# Yield vs temp and mol_ratio (time = 0)\n","ax[2].scatter(AFc_np[temp_mask][:, 1], AFc_np[temp_mask][:, 2], c=AFc_np[temp_mask][:, 3], cmap=cmap_temp, vmin=min_yield, vmax=max_yield)\n","ax[2].set_xlabel('Scaled Temp')\n","ax[2].set_ylabel('Scaled Mol Ratio')\n","ax[2].set_title('Scaled Time = 0')\n","ax[2].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"C4yZSTN3_cKc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Or we can look at slices of the filled versions (note, I have sliced it differently below):"],"metadata":{"id":"J6MyLJgY_Che"}},{"cell_type":"code","source":["# Initialize the grid points\n","grid_x = np.linspace(AFc_np[:,0].min(), AFc_np[:,0].max(), num=50)\n","grid_y = np.linspace(AFc_np[:,1].min(), AFc_np[:,1].max(), num=50)\n","\n","# Initialize for the three slices\n","slices = [-1, 0, 1]\n","\n","# Making a subplot with 1 row and 3 columns\n","fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n","\n","contour_plots = []\n","\n","# Compute global min and max of the yield\n","# This way, the same color scale will be used for all of your contour plots.\n","min_yield = np.nanmin(AFc_np[:, 3])\n","max_yield = np.nanmax(AFc_np[:, 3])\n","\n","# Loop over slices and create a contour plot for each slice\n","for i, slc in enumerate(slices):\n","    # Create a mask for the slice\n","    mask = np.isclose(AFc_np[:,2], slc, rtol=1e-5)\n","\n","    # Apply the mask to get the points and values for this slice\n","    points_slc = AFc_np[mask, :2]   # x1, x2 coordinates\n","    values_slc = AFc_np[mask, 3]    # yields\n","\n","    # Create a grid for the slice\n","    grid_slc_x, grid_slc_y = np.meshgrid(grid_x, grid_y)\n","\n","    # Interpolate the values for the grid\n","    grid_slc_z = griddata(points_slc, values_slc, (grid_slc_x, grid_slc_y), method='linear')\n","\n","    # Plot the Contour for this slice\n","    contour = axs[i].contourf(grid_slc_x, grid_slc_y, grid_slc_z, levels=20, cmap=cmap_temp, vmin=min_yield, vmax=max_yield)\n","    contour_plots.append(contour)\n","    axs[i].set_title(f'Slice at scaled mol ratio = {slc}')\n","    axs[i].set_xlabel('Scaled Temperature')\n","    axs[i].set_ylabel('Scaled Time')\n","\n","# Adding color bar\n","fig.subplots_adjust(right=0.8)\n","cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7]) # Position of colorbar [left, bottom, width, height]\n","fig.colorbar(contour_plots[1], cax=cbar_ax, label='Yield')\n","\n","plt.show()"],"metadata":{"id":"oVdGq5v-BMsv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, does this tell us the exact correct parameters to use? Not necessarily, but it does tell us where we might want to conduct extra experiments to confirm we are using the optimal parameters. We can see what regions of our parameter space are unlikely to be worth look at further and what regions are worth optimizing in.\n","\n","Could you apply this framework to the Germanium paper?\n","\n","The optimization scheme done above is extremely general, and has applications across STEM. Visualizations for higher order parameter spaces get tricky though - this example had three variables, so we could make 3D plots or 2D slides. When you have higher dimensional spaces, this gets a bit trickier."],"metadata":{"id":"rufvvhxkXGOn"}}]}