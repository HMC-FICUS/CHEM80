{"cells":[{"cell_type":"markdown","source":["# CHEM 60 - April 10th, 2024 (Principal component analysis)\n","\n","Principal Component Analysis is another scientific computing classic - it goes by many names (Singular Value Decomposition, Eigenvalue decomposition, Empirical orthogonal functions, etc.) and is used widely across pretty much all fields that deal with numerical data.\n","\n","![](https://kavassalis.space/s/Dimensions_PCA_pubs_chart.png)\n","\n","Look at the diversity of fields here!\n","\n","\n","The original dataset I wanted to use for this week's class was to be from this delightful chemistry paper: [Reviving degraded colors of yellow flowers in 17th century still life paintings with macro- and microscale chemical imaging](https://www.science.org/doi/10.1126/sciadv.abn6344). It involved x-ray powder diffraction data, image processing, PCA, and k-means clustering! Sadly, the raw data was not as available as I had hoped, but it's still a great example of this technique being applied to a - perhaps unexpected - chemical problem. We will look at one chemical application below, but there are lots of different applications out there (e.g. some [cool paleo chemistry thing?](https://www.science.org/doi/10.1126/sciadv.aba6883) or [studying novel environmental contaminants](https://www.science.org/doi/10.1126/science.aba7127) or for [drug discovery](https://www.science.org/doi/10.1126/sciadv.aao1551)\n","\n","Even the `RDkit` library you used on Monday has a [tutorial](https://chem-workflows.com/content/PCA_compounds.html) for applying PCA to chemical data because it is so routine.\n","\n","\n","---\n","\n","\n","For extra fun, given that the last couple of weeks, we have been thinking of how chemistry can inform computation (not just computation informing chemistry), here is a link to a delightful advance in PCA - [resonant quantum PCA]( https://www.science.org/doi/10.1126/sciadv.abg2589). While PCA is an old and well established technique, new variants are still being developed! This one is using insights from chemistry to build a better algorithm to do it!\n","\n","---\n","\n","Save your in-class notebook copy in your personal Drive as usual.\n"],"metadata":{"id":"LGpqCfkHAu9m"}},{"cell_type":"markdown","metadata":{"id":"Q_h-NNX5OpZ5"},"source":["#Imports\n","\n","Here are the Python imports that we will need today. We are using more pre-built functions than usual today. The default formatting stuff is here too.\n","\n","Run the below code block to get started."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Jcu7W31OpkR"},"outputs":[],"source":["# Standard library imports\n","import math as m\n","\n","# Third party imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns # for a faster plot one time\n","from sklearn.cluster import KMeans # for pre-built clustering\n","from sklearn.decomposition import PCA # and PCA\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # and normalization\n","\n","\n","# This part of the code block is telling matplotlib to make certain font sizes exra, extra large by default\n","params = {'legend.fontsize': 'xx-large',\n","         'axes.labelsize': 'xx-large',\n","         'axes.titlesize':'xx-large',\n","         'xtick.labelsize':'xx-large',\n","         'ytick.labelsize':'xx-large'}\n","# This line updates the default parameters of pyplot (to use our larger fonts)\n","plt.rcParams.update(params)"]},{"cell_type":"markdown","metadata":{"id":"VHVxu-uGgFKq"},"source":["We are going to use real experimental data today, so we'll need access to the Google Drive."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"_oEiv4qIX7MY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYM6htFuyoML"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","source":["# What is Principal Component Analysis\n","\n","Principal Component Analysis (PCA) is a technique for simplifying high-dimensional data (i.e., a dataset with lots of different variables) while maintaining patterns. It is one of the most widely used *unsupervised machine learning* algorithms out there and is extremely powerful. The goal of PCA is to capture the data's most essential features and represent the data in lower dimensions, which are called the principal components.\n","\n","To appreciate the math of PCA, you need a lot more linear algebra than we'd have time to cover in a course like this (ðŸ¥²), but we'll go over the high-level way that PCA works (and use pre-built libraries today to actually do the calculations).\n","\n","Taking data from higher to lower dimensions is kind of analogous to projecting the shadows of a three-dimensional object onto a two-dimensional surface (if you have read the 1884 hit, 'Flatland', this always comes to mind for me). You get the benefit of simplicity in a lower dimension (easy to make 2D plots! pretty much impossible to make 32D plots!), but there's a trade-off as some details are unavoidably lost in the process.\n","\n","PCA begins by determining the **hyperplane** closest to the data and then simply projects the data onto it. What a fun and obvious sentence! Let's unpack that a bit... First, think of a hyperplane as a flat, infinitely extending 'space' within the bigger space where our data lies. In 3 dimensions, a hyperplane is just a usual, flat 2-dimensional plane; in 2 dimensions, it's a one-dimensional line. This 'hyperplane' term is used to describe such flat subspaces within spaces of any dimension (we could define a 10D 'flat surface' within our hypothetical 32D space, for example). The 2D and 1D spaces are easy to picture, the higher dimensional ones less so, but the concept is the same.\n","\n","Now, the 'closest' hyperplane to our data is the plane where-  if we were to project our 'shadow' of our data onto it -  the 'shadow' would retain as much variation from the original data as possible (i.e. if you want the shadow of your hand to look like a hand and not a blob, only some angles of light and surfaces for the light to fall on work <- this analogy makes sense in my head, at least).\n","\n","Being 'closest' means minimizing the sum of squared distances from each data point to the hyperplane (jargony, yes, but you are familar with least squares methods!). The aim is to lose as little meaningful information as possible in transitioning from a higher- to a lower-dimensional space.\n","\n","So, when we say, \"PCA begins by determining the hyperplane closest to the data and then simply projects the data onto it,\" we mean that PCA starts by finding a more straightforward representation of our complex data, which sacrifices the least important information.\n","\n","The **first principal component** is the direction in the new multi-dimensional space (hyperplane) along which the data varies the most.\n","\n","![PCA demo from wikepedia - a scatter plot showing most of the data aligned](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1920px-GaussianScatterPCA.svg.png)\n","\n","Look at the above image (from Wikipedia) for example. We could describe each data point in this scatter plot by its x and y coordinate, but if we made a new axis through the main diagonal (the longer arrowed line), just its position on that line (so now just one coordinate) would tell us a whole lot about where it is within the dataset (because most of the variance is along that line).\n","\n","Once determined, the second principal component is calculated within the sub-space perpendicular to the first. Again, it's the next direction along which the data shows the most considerable variance or spread. This process continues for as many components as we have in our original data.\n","\n","This tweet (from old Twitter) is an extremely memorable and more fun way to think about it than the above scatter plot.\n","\n","![Tweet from Allison Horst saying: \"As a warm-up exercise to teach PCA I ask students to pretend they're a whale shark then ask them what angle they'd tilt their shark face if they were approaching a delicious krill swarm...\"](https://kavassalis.space/s/allison_horst_PCA.png)\n","\n","What angle to turn your whale shark mouth will capture the most krill?\n","\n","![Tweet from Allison Horst showing krill on a diagonal\"](https://kavassalis.space/s/allison_horst_PCA1.png)\n","\n","That is your first component!\n","\n","Doing this process multiple times results in a set of orthogonal axes, the principal components, in descending order of the variance of data. This new 'coordinate system' represents the same data, i.e., it repeats the basic *story* from the dataset but with fewer \"words\", or in this case, fewer dimensions.\n","\n","Okay, that might have been a lot of reading. Let's jump to seeing what it does."],"metadata":{"id":"uJaBnEHifj1G"}},{"cell_type":"markdown","source":["# Lost in chemical space?\n","\n","I love a good story. We're going to do our last class paper reading together - this is a review paper, but sets up nicely how PCA can be used to help discover novel chemsitry.\n","\n","Open up \"[Lost in chemical space? Maps to support organometallic catalysis](https://bmcchem.biomedcentral.com/articles/10.1186/s13065-015-0104-5)\" by Natalie Fey and the Google Doc for class note taking.\n","\n","After we've read and shared, we'll work through an example the author shows in this paper."],"metadata":{"id":"fHVfoyRrYAJS"}},{"cell_type":"markdown","source":["## Load the data\n","\n","Let's go through the process of loading a new data frame as a pandas object. This is the data referenced in Fig. 2 (originally from the Jover et al. 2010 paper).\n","\n"],"metadata":{"id":"Dx91LIslYTSg"}},{"cell_type":"code","source":["# Jover J, Fey N, Harvey JN, Lloyd-Jones GC, Orpen AG, Owen-Smith GJJ, et al. Expansion of the ligand knowledge base for monodentate P-donor ligands (LKB-P). Organometallics. 2010;29:6245â€“58.\n","df = pd.read_csv('/content/gdrive/Shared drives/Chem_60_Spring_2024/In_Class_Notebooks/data/CHEM60_Class22_data_Jover_etal.csv')\n","df.head()"],"metadata":{"id":"R97vw-nlYU54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay. First thing to notice - pandas wants data frames to have an index - something that orders the data points. If you don't tell it what the index is, it'll make one for you (that's the left-most column)."],"metadata":{"id":"QP1hrdMCZA1A"}},{"cell_type":"code","source":["df.index"],"metadata":{"id":"i7INTvq4ZLTB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This data comes with it's own though (that happens to look extremely similar to the one pandas assigned). Let's use the **No.** column for index as the authors intended."],"metadata":{"id":"0TZvV56_ZNLH"}},{"cell_type":"code","source":["df.set_index('No.', inplace=True)\n","df.head()"],"metadata":{"id":"hGSN6O_GZZnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also can notice that there is a column that shouldn't be there - the far right column must have been loaded by mistake (ie. it has no name, and is full of NaNs). Let's get rid of it."],"metadata":{"id":"do4Gxm8VZ7Do"}},{"cell_type":"code","source":["df = df.drop(['Unnamed: 33'], axis=1)\n","df.head()"],"metadata":{"id":"Ue-2pnBDaF_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Good practice says, before we *use* data, we look at it! This data is a bit hard to visualize though given it's size (so many different variables to plot - I don't personally find 32-dimensional spaces easy to visualize).\n","\n","We also might not intuitively know what it *should* look like. Ideally, we should have some expectations going in when working with data - should some variables have some sort of a relationship? What kind of distributions might we expect?\n","\n","HOMO (highest occupied molecular orbital) and LUMO (lowest unoccupied molecular orbital) energy should have *some* relationship, for example, so let's look at them."],"metadata":{"id":"Q16L8lVqZzBo"}},{"cell_type":"code","source":["fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # Create a subplot with 1 row, 2 columns\n","\n","# On the left plot, we draw the scatter plot\n","axs[0].scatter(df['E(LUMO)'], df['E(HOMO)'], edgecolor='k')\n","axs[0].set_ylabel('E(HOMO) (units)')\n","axs[0].set_xlabel('E(LUMO) (units)')\n","axs[0].set_ylim(min(df['E(LUMO)'].min(), df['E(HOMO)'].min())-0.05, max(df['E(LUMO)'].max(), df['E(HOMO)'].max())+0.05)\n","axs[0].set_xlim(min(df['E(LUMO)'].min(), df['E(HOMO)'].min())-0.05, max(df['E(LUMO)'].max(), df['E(HOMO)'].max())+0.05)\n","xlims = axs[0].get_xlim(); axs[0].plot(xlims, xlims, color='k', linestyle='--', linewidth=2) # adding a 1-1 line\n","\n","# On the right plot, we draw the histogram\n","axs[1].hist(df['E(HOMO)'], bins=10, edgecolor='k')\n","axs[1].set_xlabel('E(HOMO) (units)')\n","axs[1].set_ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"4MvxANTlaPGM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above looks reasonable-ish to me (ie. the points are below the one-to-one line, vauge relationship between variables we think should have a relationship, distributions aren't normal, but not bimodal or something else unexpected).\n","\n","Ideally, you should work with data you know a lot about. If not, work with someone who knows a lot about it! Computation is powerful when coupled with conceptual understanding - these need to go together (because sometimes, a dataset is nonsense!).\n","\n","Okay, mini rant over."],"metadata":{"id":"3diIoPkiak3a"}},{"cell_type":"markdown","source":["#1. Data Preprocessing\n","\n","Once we feel okay about using the data, the next thing to check is if we are missing any values. It is easy to intentionally or unintentionally have NaNs (not-a-numbers) in a dataset. We just need to know before we use the data. PCA will require all of our variables to be numeric (floats or ints, without missing values)."],"metadata":{"id":"23OP19QzeatT"}},{"cell_type":"code","source":["df.isnull().values.any()"],"metadata":{"id":"AhpCm_l3ZzJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, so that means no NaNs present. Next step, normalize the data."],"metadata":{"id":"7zJXWyWWtN5s"}},{"cell_type":"markdown","source":["## Normalization\n","\n","**Why do we need to normalize the data?** Well, for starters, we don't want units. But, the reasons are bigger than that. Imagine you're working with a data set that includes measurements in a mixture of units, for instance, millimetres and kilometres. You might want to put them all to the same units initially, but these differing scales could lead to biased results in the PCA owing to the simple fact that a kilometre is big and a millimetre is small. If you measured them in different units, they are likely going to be very different orders of magnitude when in the same units. This implies that without normalization, PCA might determine the direction of maximum variance based on the unit having larger variances when it's really relative variance that matters. Another way I have heard this explained that I kind of like - PCA may be unfairly influenced by the 'loud voices' in the data, while the 'quieter voices' might get overlooked. Normalization ensures that all 'voices' are heard at the same volume, leading to a more balanced and accurate analysis.\n","\n","There are actually a couple common ways to normalize data (what? not just one? no...) Let's look at three different approaches used in the data sciences."],"metadata":{"id":"tbeJNHIyeNYA"}},{"cell_type":"markdown","source":["### Standard Scaler\n","\n","This is a commonly used normalization scheme, but perhaps not the 'standard' one you'd think about. Read up on it [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler). While this normalization method is very simple to write yourself (look at the formula!), we are going to use the pre-built function to do it to save time today."],"metadata":{"id":"9qBxpuZNqzvt"}},{"cell_type":"code","source":["scaler = StandardScaler()\n","df_StandardScaler = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n","df_StandardScaler.head()"],"metadata":{"id":"pHzCXmjKq39E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Do these numbers look like what you would have expected normalized numbers to look? Why or why not?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"AnFgJzr3r1Kl"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"93KtwiGsr-cs"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"FTgsyf5XsAVu"}},{"cell_type":"markdown","source":["Okay, but there are more options!"],"metadata":{"id":"7wClqJPRruRF"}},{"cell_type":"markdown","source":["### Min-Max Scaler\n","\n","Check out this one now, [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)"],"metadata":{"id":"qaCscI8arxI1"}},{"cell_type":"code","source":["scaler = MinMaxScaler()\n","df_MinMaxScaler = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n","df_MinMaxScaler.head()"],"metadata":{"id":"56dhfAwVsL5k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","Double check these two methods give... related variables. How could we check? What visualizations or statistics could help us understand the relationship between the Standard Scalar and Min-Max Scalar?\n","\n","\n","---\n","\n"],"metadata":{"id":"xjfIvhpjsRlt"}},{"cell_type":"code","source":["# code?"],"metadata":{"id":"BzPgcluLsmUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"FXUiZguOslC9"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"HyzQxbVnsoUM"}},{"cell_type":"markdown","source":["### Robust Scaler\n","\n","The final commonly used method is the Robust Scaler, described [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)."],"metadata":{"id":"rpzOJV0KsYj0"}},{"cell_type":"code","source":["scaler = RobustScaler()\n","df_RobustScaler = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n","df_RobustScaler.head()"],"metadata":{"id":"gShHc8nAszuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In your homework, you'll have more time to reflect on these three methods and why some datasets might be better served by one or the other.\n","\n","For now, we'll do the following examples with the Standard Scalar (it is... \"standard\", afterall)."],"metadata":{"id":"uQNxaohgsYrt"}},{"cell_type":"markdown","source":["# 2. PCA time\n","\n","While we are going to use the pre-built function to do this, I will just talk through the basic math this is going on. If these terms are totally unfamilar, focus on qualitative understanding.\n","\n","**Covariance Matrix computation**\n","\n","The first thing calculated is a covariance matrix of the data to understand how different variables move together. The covariance matrix is a `p x p` matrix where each element represents the covariance between two features (`p` total features).\n","\n","$$ cov(X, Y) = \\frac{\\sum{(x_i - \\mu_x)(y_i - \\mu_y)}}{n-1} $$\n","\n","where:\n","- $(X)$ and $(Y)$ are two variables,\n","- $(x_i)$ and $(y_i)$ are the observations for each variable,\n","- $(\\mu)$ is the mean, and\n","- $(n)$ is the number of data points.\n","\n","We can see what the matrix looks like for this data set by letting `numpy` calculate it for us."],"metadata":{"id":"kam7knevtJbT"}},{"cell_type":"code","source":["cov_matrix = np.cov(df_StandardScaler, rowvar=False)"],"metadata":{"id":"bbggAm8nujq8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A common way to visualize things like this is through a plot called a heat map. To make this code tidy, we'll use Seaborn (`sns`)'s prebuilt function (you can make them in matplotlib too, but the code block would be bigger)."],"metadata":{"id":"PDjDlx4Hulzk"}},{"cell_type":"code","source":["plt.figure(figsize=(20, 20))\n","sns.heatmap(cov_matrix, annot=True, fmt='.2f')\n","plt.show()"],"metadata":{"id":"11iRe1JJuJS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we can see positive and negative correlations between the different variables. Seeing large magnitude correlations tells us PCA will be very effective. Why? Because correlated variables essentially tell us we have redundant data being stored here (PCA is a compression algorithm!). If two or more variables are highly correlated with each other, what if we just stored one variable that roughly captured the variance in both? That's PCA for you."],"metadata":{"id":"p5dAqBfzvHyE"}},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","While Seaborn is very popular for it's better prebuilt colour maps, the default one for heatmaps (above) is actually *bad* for correlation matrices. Why? What qualities of a colour map would be better for the above? You can learn about the heatmap syntax [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html) and a reminder [link](https://colorbrewer2.org/) to a good resource for thinking about colour maps.\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"KzonrLfnveo5"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"Z1IXZxmPwLb7"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"WsPdxi23wPm7"}},{"cell_type":"markdown","source":["\n","The next thing the PCA algorithm does is **compute the Eigenvalues and Eigenvectors**. The eigenvectors represent the direction of the new sub-space (angle of shark face), and eigenvalues represent their magnitude (how wide a mouth of shark). We are not going to step through this math because it would take a couple classes on its own. Once we have the eigenvalues and eigenvectors, we **sort** them in decreasing order and choose the first few that contain the most information (variance). These comprise the new dimensions. Finally,**the original data is transformed onto the dimension that we choose**, generating a new, reduced dataset.\n","\n","We'll let [Scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) take the wheel today.\n","\n","Some choices need to be made though.\n","\n","1. What normalization scheme makes sense for the data (we said we'd use the Standard Scalar here)\n","2. How many components should we pick?\n","\n","Let's pick all of them! (ie. N = 32)\n","\n"],"metadata":{"id":"6q5bi8qyucpM"}},{"cell_type":"code","source":["pca = PCA(n_components=32)\n","principalComponents = pca.fit_transform(df_StandardScaler)"],"metadata":{"id":"2168Bn_kucSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wow, what did that do?\n","\n","Let's ask for the amount of variance associated with each component (ie. how much of the data is it really describing). Using the pre-built function is nice, because `explained_variance_ratio_` is just an [attribute](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) of our `pca` object."],"metadata":{"id":"MyFj-ehQxqQ7"}},{"cell_type":"code","source":["explained_variance = pca.explained_variance_ratio_\n","explained_variance"],"metadata":{"id":"DKjtxTu0zJa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Scree Plot\n","It might be hard to see what is important from just looking at those numbers, so let's plot it (this is called a Scree Plot)!"],"metadata":{"id":"Yogp_-95zrdS"}},{"cell_type":"code","source":["components = range(1, len(principalComponents[1])+1) # this is just to label our components\n","\n","plt.figure(figsize=(14, 4))\n","plt.plot(components, explained_variance, marker='o')\n","plt.ylim([0,max(explained_variance)+.05])\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal components')\n","plt.xticks(components)\n","plt.show()"],"metadata":{"id":"xv-j-XT6zq66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What does this tell us? Well, that we could have picked a much smaller number! Why? Let's look at those numbers."],"metadata":{"id":"R1oKcTnBzuOC"}},{"cell_type":"code","source":["print(\"The First PC is responsible for \" + str(round(explained_variance[0]*100,3)) + \"% of the variance\")"],"metadata":{"id":"7FohQ1mv0Aur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The First 2 PCs are responsible for \" + str(round((explained_variance[0]+explained_variance[1])*100,3)) + \"% of the variance\")"],"metadata":{"id":"70E3r5QZ0S37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The First 3 PCs are responsible for \" + str(round((explained_variance[0]+explained_variance[1]+explained_variance[2])*100,3)) + \"% of the variance\")"],"metadata":{"id":"0DWL2GCL0ctb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The First 4 PCs are responsible for \" + str(round((explained_variance[0]+explained_variance[1]+explained_variance[2]+explained_variance[3])*100,3)) + \"% of the variance\")"],"metadata":{"id":"Xj6yuOtu0eNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The 5th PC would only add \" + str(round((explained_variance[4])*100,3)) + \"% of the variance\")"],"metadata":{"id":"5uXNlrkh0neD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, most of the variance in the data is captured by the first 4 components and the rest have very small contributions. We could also just re-do the PCA with just 4 components and the variance explained is the same."],"metadata":{"id":"XfYs10Af0YFz"}},{"cell_type":"code","source":["pca = PCA(n_components=4)\n","principalComponents = pca.fit_transform(df_StandardScaler)\n","explained_variance = pca.explained_variance_ratio_\n","explained_variance"],"metadata":{"id":"iBSG0Zqm05bb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Let's plot the data\n","\n","\n","First, let's make a pandas dataframe to store the transformed dataset so it's easier to plot. We can also wrap our head's around what the data looks like now - no physically descriptive variable names, just our four components.\n"],"metadata":{"id":"6TXnkHcA1SpL"}},{"cell_type":"code","source":["df_PCA = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n","df_PCA"],"metadata":{"id":"5vc6iJBm1ZLr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's plot 2 of the dimensions."],"metadata":{"id":"qHxPgfFa1-xb"}},{"cell_type":"code","source":["plt.scatter(df_PCA['PC1'], df_PCA['PC2'], edgecolor='k', s=50)\n","plt.xlabel('PC1'); plt.ylabel('PC2')\n","plt.show()"],"metadata":{"id":"ABpjyzMG1-6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well, this is honestly... not hugely informative on its own. We still have a 4D space, so this 2D plot doesn't capture everything, but it actually is a way better visualization of the entire dataset than just plotting the HOMO and LUMO energy above (those were 2 our of 32 variables), PC1 and PC2 are 2 variables that represent >60% of the variance in the entire dataset (that's a big change).\n","\n","What if we could say which data points were most alike in this transformed dataset? How can we use this new information to better explore the chemical space we're interested in?"],"metadata":{"id":"K0xyiFkp2VnH"}},{"cell_type":"markdown","source":["# K-means Clustering\n","\n","The hero of so many problems, the humble K-means clustering algorithm is back to help us make meaning of this data. Two assignments so far have included the code to do this ourselves, but we'll use scikit learn's [function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) today.\n","\n","First, we need an initial guess for the number of clusters in the data (how many genres of *things* are represented here?). It is okay to take a guess at this and try different values until one makes sense (ie. will it group things that have some chemical reason for being in a group together or not? Understanding the underlying meaning of data is always key)."],"metadata":{"id":"u5BEscVI2f89"}},{"cell_type":"code","source":["n_clusters = 5\n","kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n","y_kmeans = kmeans.fit_predict(df_PCA) # what does this line do? Look at the documentation!\n","centres = kmeans.cluster_centers_ # what does this line do?"],"metadata":{"id":"1_qW9vwF3GPC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION\n","\n","What are the above lines of code returning? Why? Read the Scikit learn documentation.\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"1xglgN3Q35Xj"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"W-N-qDtz3_fL"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","As always, plots help."],"metadata":{"id":"UsEKFK9e4BLi"}},{"cell_type":"code","source":["plt.scatter(df_PCA['PC1'], df_PCA['PC2'], c=y_kmeans, edgecolor='k', s=50, cmap='viridis')\n","plt.scatter(centres[:, 0], centres[:, 1], c=range(n_clusters), marker = \"*\", s=1000, alpha=0.5);\n","plt.xlabel('PC1'); plt.ylabel('PC2')\n","plt.show()"],"metadata":{"id":"W2wJGxit2-D7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How do we interpret this? We have to know what the members of the different groups have in common! Let's put this information back with our original dataframe."],"metadata":{"id":"YuQ0CPwM7YwE"}},{"cell_type":"code","source":["df['Cluster'] = y_kmeans\n","df.head()"],"metadata":{"id":"yuapgH7X79hB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One really helpful piece of information that isn't in this original dataframe - coded information on the chemical structures that each of these entries is describing! This could be the name of a ligand or some other chemical descriptor. This kind of information is commonly included in the form of SMILES (you just learned about these)!"],"metadata":{"id":"sE5Vv-Ae8NSi"}},{"cell_type":"markdown","source":["## Getting back to the meaning of the components\n","\n","But what if we want to know what the different components physically represent? Well, they represent a combination of original variables. We can see what combination if we want."],"metadata":{"id":"mJ_U7lOJ4oUT"}},{"cell_type":"code","source":["features = df_StandardScaler.columns\n","components_df = pd.DataFrame(pca.components_, columns=features, index=[f'PCA{i+1}' for i in range(4)])\n","components_df.head()"],"metadata":{"id":"gGfm_hYN5vd7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, the values indicate how much each original feature contributes to the PC. The sign of weights (values in the above dataframe) represent the direction of the contribution. A positive weight means the feature and the PC are directly proportional, i.e., when the feature's value increases, the PC's value also increases. A negative weight means they are inversely proportional; as the feature's value increases, the PC's value decreases. Sometimes a component will be mostly one or two variables, sometimes a mix of everything.\n","\n","If you square these values and sum them up for each row (each principal component), they should add up to 1. This is because each principal component is a unit vector, and the sum of squares of a unit vector's components is equal to 1.\n","\n","We can test it out."],"metadata":{"id":"Oho2GvmB6Ca5"}},{"cell_type":"code","source":["for i, sum_of_squares in enumerate(components_df.pow(2).sum(axis=1), start=1):\n","    print(f'The sum of squares for PCA{i} is {sum_of_squares}')"],"metadata":{"id":"VIIIk-uG6Bzi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PRACTICE QUESTION?\n","\n","Are these numbers exactly 1? Why/why not?\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"YI9LpqJy7Nrj"}},{"cell_type":"markdown","source":["**notes**"],"metadata":{"id":"GQLtKRFg7e4a"}},{"cell_type":"markdown","metadata":{"id":"SvE2DJP950R2"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","source":["In some, but not many, applications, assigning pseudo-physical meaning to the different principal components is important. In many (including this one), it's really not. The components are a mix of all the features. The insights come from being able to cluster our data in the new space.\n","\n","\n","Technically, you can perform K-means clustering on the original data, however, performing clustering after PCA has *significant* benefits.\n","\n","1. **Noise reduction**: PCA can help remove noise from the data. Noise (random fluctuations in the data) almost unintentionally gets lost when you do PCA. Unless the noise comes from a systematic error source, it'll get cancelled out!\n","\n","2. **Reduction of redundancy**: We saw with the covariance matrix that many features in our original data were highly correlated with each other so don't actually add much new information individually. Correlated features make clustering (and many types of learning) hard (if you have 10 pieces of information that are essentially the same and 2 that are unique, you run the risk of overemphasizing the 10). PCA means that every direction is equally important and unique, leading to more objectively shaped clusters.\n","\n","3. **Computational efficiency**: Reducing the number of dimensions with PCA can greatly speed up clustering algorithm, especially on large datasets (this data set we are looking at is small - but this approach can be applied on Big data too).  \n","\n","4. **Visualization**: One of the most significant benefits of PCA is that it allows you to visualize high-dimensional data. After PCA, you can plot the first two or three principal components and visualise the clusters, which, assuming the first couple of components explain most of the variance in the data, can let you see a whole lot.\n","\n","If your original data is not high dimensional, or if the features are highly distinctive (low correlation), applying PCA might not be necessary - while it's a great tool, not every problem is the right fit (this is true for... all tools).\n","\n","\n","---\n","\n","Your homework will let you dig in more on this data set and PCA.\n"],"metadata":{"id":"hv1paxf_8uXa"}},{"cell_type":"markdown","metadata":{"id":"y5rNn97ftVIO"},"source":["# Submit your notebook\n","\n","It's time to download your notebook and submit it on Canvas. Go to the File menu and click **Download** -> **Download .ipynb**\n","\n","Then, go to **Canvas** and **submit your assignment** on the assignment page. Once it is submitted, swing over to the homework now and start working through the paper."]}],"metadata":{"colab":{"provenance":[{"file_id":"1qHpJsyiLZKDQSILs6YS0oKg9DJCzCA7T","timestamp":1712739179155},{"file_id":"16FmG2mSVcRimbRRivcxXuco3J_QbxtJe","timestamp":1712137606278},{"file_id":"1udD5v1As1QrhRPmfJgzPRdTx992n7sgn","timestamp":1710921034178},{"file_id":"1CDW0YYejxxBwaBHZ3yvlCrF9jP_xOlFB","timestamp":1709552969718},{"file_id":"1KJVhs9Wmx7ff7NyPytj8p9v7u9Zmbhgx","timestamp":1708040382739},{"file_id":"17cUJAmt-Ze--6OfutYvzwEV3Go0C1b9e","timestamp":1706638038791},{"file_id":"1-osvtQCGVecL-32l3fHgJmn2ONyfjHqn","timestamp":1696907647526}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}